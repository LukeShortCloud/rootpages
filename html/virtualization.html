<h1 id="virtualization">Virtualization</h1>
<ul>
<li><a href="#hardware-virtualization">Hardware Virtualization</a>
<ul>
<li><a href="#hardware-virtualization---kvm">KVM</a>
<ul>
<li><a href="#hardware-virtualization---kvm---performance-tuning">Performance Tuning</a></li>
<li><a href="#hardware-virtualization---kvm---nested-virtualization">Nested Virtualization</a></li>
<li><a href="#hardware-virtualization---kvm---gpu-passthrough">GPU Passthrough</a></li>
<li>virt-manager</li>
<li>oVirt</li>
</ul></li>
<li><a href="#hardware-virtualization---xen">Xen</a>
<ul>
<li><a href="#hardware-virtualization---xen---nested-virtualization">Nested Virtualization</a></li>
<li>XenServer</li>
</ul></li>
</ul></li>
<li><a href="#software-virtualization">Software Virtualization</a>
<ul>
<li>QEMU</li>
<li><a href="#software-virtualization---containers">Containers</a>
<ul>
<li><a href="#software-virtualization---containers---docker">Docker</a>
<ul>
<li><a href="#software-virtualization---containers---docker---networking">Networking</a></li>
</ul></li>
<li><a href="#software-virtualization---containers---lxc">LXC</a></li>
</ul></li>
</ul></li>
<li><a href="#orchestration">Orchestration</a>
<ul>
<li><a href="#orchestration---vagrant">Vagrant</a>
<ul>
<li><a href="#orchestration---vagrant---vagrantfile">Vagrantfile</a>
<ul>
<li><a href="#orchestration---vagrant---vagrantfile---networks">Networks</a>
<ul>
<li><a href="#orchestration---vagrant---vagrantfile---networks---libvirt">libvirt</a></li>
</ul></li>
<li><a href="#orchestration---vagrant---vagrantfile---provisioning">Provisioning</a></li>
<li><a href="#orchestration---vagrant---vagrantfile---multiple-machines">Multiple Machines</a></li>
</ul></li>
</ul></li>
<li>Terraform</li>
</ul></li>
</ul>
<h1 id="hardware-virtualization">Hardware Virtualization</h1>
<p>Hardware virtualization uses specially supported processors to speed up and isolate virtualized environments. Most newer CPUs support this. There is &quot;Intel VT (Virtualization Techonology)&quot; and &quot;AMD SVM (Secure Virtual Machine)&quot; for x86 processors. [1]</p>
<p>Intel has three subtypes of virtualization:</p>
<ul>
<li>VT-x = Basic hardware virtualization and host separation support.</li>
<li>VT-d = I/O passthrough support.</li>
<li>VT-c = Improved network I/O passthrough support.</li>
</ul>
<p>[2]</p>
<p>AMD has two subtypes of virtualization:</p>
<ul>
<li>AMD-V = Basic hardware virtualization and host separation support.</li>
<li>AMD-Vi = I/O passthrough support.</li>
</ul>
<p>Check for Intel or AMD virtualization support:</p>
<pre><code>$ grep vmx /proc/cpuinfo # Intel</code></pre>
<pre><code>$ grep svm /proc/cpuinfo # AMD</code></pre>
<p>Verify the exact subtype of virtualization:</p>
<pre><code>$ lscpu | grep ^Virtualization # Intel or AMD</code></pre>
<p>Hardware virtualization must be supported by both the motherboard and processor. It should also be enabled in the BIOS. [1]</p>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;Linux: Find Out If CPU Support Intel VT and AMD-V Virtualization Support.&quot; February 11, 2015. nixCraft. Accessed December 18, 2016. https://www.cyberciti.biz/faq/linux-xen-vmware-kvm-intel-vt-amd-v-support/</li>
<li>&quot;Intel VT (Virtualization Technology) Definition.&quot; TechTarget. October, 2009. Accessed December 18, 2016. http://searchservervirtualization.techtarget.com/definition/Intel-VT</li>
</ol>
<h2 id="hardware-virtualization---kvm">Hardware Virtualization - KVM</h2>
<p>The &quot;Kernel-based Virtual Machine (KVM)&quot; is the default kernel module for handling hardware virtualization in Linux since the 2.6.20 kernel. [1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Kernel Virtual Machine.&quot; KVM. Accessed December 18, 2016. http://www.linux-kvm.org/page/Main_Page</li>
</ol>
<h3 id="hardware-virtualization---kvm---performance-tuning">Hardware Virtualization - KVM - Performance Tuning</h3>
<p>Configuration detials for virtual machines can be modified to provide better performance. For processors, it is recommended to use the same CPU settings so that all of it's features are available to the guest. [1]</p>
<pre><code># qemu -cpu host ...</code></pre>
<pre><code># virsh edit &lt;VIRTUAL_MACHINE&gt;
&lt;cpu mode=&#39;host-passthrough&#39;&gt;</code></pre>
<p>The network driver that provides the best performance is &quot;virtio.&quot; Some guests may not support this feature and require additional drivers.</p>
<pre><code># qemu -net nic,model=virtio ...</code></pre>
<pre><code># virsh edit &lt;VIRTUAL_MACHINE&gt;
&lt;interface type=&#39;network&#39;&gt;
  ...
  &lt;model type=&#39;virtio&#39; /&gt;
&lt;/interface&gt;****</code></pre>
<p>Using a tap device (that will be greated to an existing interface) or a bridge will speed up network greatly.</p>
<pre><code>... -net tap,ifname=&lt;NETWORK_DEVICE&gt; ...</code></pre>
<pre><code>... -net bridge,br=&lt;NETWORK_BRIDGE_DEVICE&gt; ...</code></pre>
<pre><code># virsh edit &lt;VIRTUAL_MACHINE&gt;
    &lt;interface type=&#39;bridge&#39;&gt;
...
      &lt;source bridge=&#39;&lt;BRDIGE_DEVICE&gt;&#39;/&gt;
      &lt;model type=&#39;virtio&#39;/&gt;
    &lt;/interface&gt;</code></pre>
<p>If possible, PCI passthrough provides the best performance as there is no virtualization overhead.</p>
<pre><code># qemu -net none -device vfio-pci,host=&lt;PCI_DEVICE_ADDRESS&gt; ...</code></pre>
<p>Raw disk partitions have the greatest speeds with the &quot;virtio&quot; driver and cache disabled.</p>
<pre><code># qemu -drive file=&lt;PATH_TO_STORAGE_DEVICE&gt;,cache=none,if=virtio ...</code></pre>
<pre><code># virsh edit &lt;VIRTUAL_MACHINE&gt;
&lt;disk type=&#39;...&#39; device=&#39;disk&#39;&gt;
  ...
  &lt;target dev=&#39;&lt;DEVICE_NAME&gt;&#39; bus=&#39;virtio&#39;/&gt;
&lt;/disk&gt;</code></pre>
<p>[1][2]</p>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;Tuning KVM.&quot; KVM. Accessed January 7, 2016. http://www.linux-kvm.org/page/Tuning_KVM</li>
<li>&quot;Virtio.&quot; libvirt Wiki. October 3, 2013. Accessed January 7, 2016. https://wiki.libvirt.org/page/Virtio</li>
</ol>
<h3 id="hardware-virtualization---kvm---nested-virtualization">Hardware Virtualization - KVM - Nested Virtualization</h3>
<p>KVM supports nested virtualization. This allows a virtual machine full access to the processor to run another virtual machine in itself. This is disabled by default.</p>
<h4 id="option-1---modprobe">Option #1 - Modprobe</h4>
<ul>
<li><p>Intel</p>
<pre><code># vim /etc/modprobe.d/nested_virtualization.conf
options intel nested=1</code></pre>
<pre><code># modprobe -r kvm-intel
# modprobe kvm-intel</code></pre></li>
<li><p>AMD</p>
<pre><code># vim /etc/modprobe.d/nested_virtualization.conf
options amd nested=1</code></pre>
<pre><code># modprobe -r kvm-amd
# modprobe kvm-amd</code></pre></li>
</ul>
<h4 id="option-2---grub2">Option #2 - GRUB2</h4>
<ul>
<li><p>Intel</p>
<pre><code># vim /etc/default/grub
GRUB_CMDLINE_LINUX=&quot;console=tty0 kvm-intel.nested=1&quot;</code></pre></li>
<li><p>AMD</p>
<pre><code># vim /etc/default/grub
GRUB_CMDLINE_LINUX=&quot;console=tty0 kvm-amd.nested=1&quot;</code></pre></li>
<li><p>Then rebuild the GRUB 2 configuration.</p>
<pre><code># grub-mkconfig -o /boot/grub/grub.cfg</code></pre></li>
</ul>
<p>[1]</p>
<p>Finally, edit the virtual machine's XML configuration to change the CPU mode to be &quot;host-passthrough.&quot;</p>
<pre><code># virsh edit &lt;VIRTUAL_MACHINE&gt;
&lt;cpu mode=&#39;host-passthrough&#39;&gt;</code></pre>
<p>[2]</p>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;How to Enable Nested KVM.&quot; Rhys Oxenhams' Cloud Technology Blog. June 26, 2012. Accessed December 18, 2016. http://www.rdoxenham.com/?p=275</li>
<li>&quot;Configure DevStack with KVM-based Nested Virtualization.&quot; December 18, 2016. Accessed December 18, 2016. http://docs.openstack.org/developer/devstack/guides/devstack-with-nested-kvm.html</li>
</ol>
<h3 id="hardware-virtualization---kvm---gpu-passthrough">Hardware Virtualization - KVM - GPU Passthrough</h3>
<p>GPU passthrough is useful for running a Windows virtual machine guest for gaming inside of Linux. It is recommended to have two video cards, one for Linux and one for the guest virtual machine. [1]</p>
<p>Nvidia cards have a detection in the driver to see if the operating system has a hypervisor running. This can lead to a &quot;Code: 43&quot; error in the driver as it false-positively reports none. This affects Nvidia drivers starting with version 337.88. A work-a-round for this is to set a random &quot;vendor_id&quot; to a alphanumeric 12 character value and forcing KVM's emulation to be hidden. This does not affect ATI/AMD graphics cards. [2]</p>
<pre><code># virsh edit &lt;VIRTUAL_MACHINE&gt;
&lt;features&gt;
    &lt;hyperv&gt;
        &lt;vendor_id state=&#39;on&#39; value=&#39;123456abcdef&#39;/&gt;
    &lt;/hyperv&gt;
    &lt;kvm&gt;
        &lt;hidden state=&#39;on&#39;/&gt;
    &lt;/kvm&gt;
&lt;/features&gt;</code></pre>
<p>[2]</p>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;GPU Passthrough with KVM and Debian Linux.&quot; scottlinux.com Linux Blog. August 28, 2016. Accessed December 18, 2016. https://scottlinux.com/2016/08/28/gpu-passthrough-with-kvm-and-debian-linux/</li>
<li>&quot;PCI passthrough via OVMF.&quot; Arch Linux Wiki. December 18, 2016. Accessed December 18, 2016. https://wiki.archlinux.org/index.php/PCI_passthrough_via_OVMF</li>
</ol>
<h2 id="hardware-virtualization---xen">Hardware Virtualization - Xen</h2>
<p>Xen is a free and open source software published under the GNU General Public License (GPL). It was originally designed to be a competitor of VMWare. It is currently owned by Citrix and offers a paid support package for it's virtual machine hypervisor/manager XenServer. [1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Xen Definition.&quot; TechTarget. March, 2009. Accessed December 18, 2016. http://searchservervirtualization.techtarget.com/definition/Xen</li>
</ol>
<h3 id="hardware-virtualization---xen---nested-virtualization">Hardware Virtualization - Xen - Nested Virtualization</h3>
<p>Since Xen 4.4, nested virtualization has been supported by default. It needs to be configured in the guest virtual machine using &quot;nestedhvm.&quot; The &quot;hap&quot; feature also needs to be enabled for better performance and stability. Lastly, the CPU's ID needs to be modified to hide the original virtualization ID.</p>
<pre><code>nestedhvm=1
hap=1
cpuid = [&#39;0x1:ecx=0xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#39;]</code></pre>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Nested Virtualization in Xen.&quot; Xen Project Wiki. December 13, 2016. Accessed December 18, 2016. https://wiki.xenproject.org/wiki/Nested_Virtualization_in_Xen</li>
</ol>
<h1 id="software-virtualization">Software Virtualization</h1>
<h1 id="software-virtualization---containers">Software Virtualization - Containers</h1>
<p>Containers are a type of software virtualization. Using a directory structure that contains an entire operating system (typically referred to as a chroot), containers can easily spin up and utilize system resources without the overhead of full hardware allocation. It is not possible to use seperate kernels with this approach.</p>
<h3 id="software-virtualization---containers---docker">Software Virtualization - Containers - Docker</h3>
<p>Docker creates containers using the LXC kernel module on Linux.</p>
<p>A command is run to start a daemon in the container. As long as that process is still running in the foreground, the container will remain active. Some processes may spawn in the background. A workaround for this is to append <code>&amp;&amp; tail -f /dev/null</code> to the command. If the daemon successfully starts, then a never-ending task can be run instead (such as viewing the never ending file of /dev/null). [1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Get started with Docker.&quot; Docker. Accessed November 19, 2016. https://docs.docker.com/engine/getstarted</li>
</ol>
<h3 id="software-virtualization---containers---docker---networking">Software Virtualization - Containers - Docker - Networking</h3>
<p>Networking is automatically bridged to the public interface and set up with a NAT. This allows full communication to/from the container, provided that the necessary ports are open in the firewall and configured in the Docker image.</p>
<p>Networking issues from within a container are commonly due to network packet size (MTU) issues. There are a few work-a-rounds.</p>
<ol style="list-style-type: decimal">
<li><p>Configure the default MTU size for Docker deployments by modifying the daemon's process settings. This value should generally be below the default of 1500.</p>
<pre><code># vim /etc/sysconfig/docker
OPTIONS=&#39;--selinux-enabled --log-driver=journald --mtu 1400&#39;
# systemctl restart docker</code></pre>
<p>OR</p>
<pre><code># vim /usr/lib/systemd/system/docker.service
ExecStart=/usr/bin/docker-current daemon \
      --exec-opt native.cgroupdriver=systemd --mtu 1400 \
      $OPTIONS \
      $DOCKER_STORAGE_OPTIONS \
      $DOCKER_NETWORK_OPTIONS \
      $ADD_REGISTRY \
      $BLOCK_REGISTRY \
      $INSECURE_REGISTRY
# systemctl daemon-reload
# systemctl restart docker</code></pre></li>
<li><p>Forward all packets between the Docker link through the physical link.</p>
<pre><code># iptables -I FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</code></pre></li>
</ol>
<p>[1]</p>
<p>In rare cases, the bridge networking will not be working properly. An error message similar to this may appear during creation.</p>
<pre><code>ERROR: for &lt;CONTAINER_NAME&gt; failed to create endpoint &lt;NETWORK_ENDPOINT&gt; on network bridge: iptables failed: iptables --wait -t nat -A DOCKER -p tcp -d 0/0 --dport &lt;DESTINATION_PORT_HOST&gt; -j DNAT --to-destination &lt;IP_ADDRESS&gt;:&lt;DESTINATION_PORT_CONTAINER&gt; ! -i docker0: iptables: No chain/target/match by that name.</code></pre>
<p>The solution is to delete the virtual &quot;docker0&quot; interface and then restart the Docker service for it to be properly recreated.</p>
<pre><code># ip link delete docker0
# systemctl restart docker</code></pre>
<p>[2]</p>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;containers in docker 1.11 does not get same MTU as host #22297.&quot; Docker GitHub. September 26, 2016. Accessed November 19, 2016. https://github.com/docker/docker/issues/22297</li>
<li>&quot;iptables failed - No chain/target/match by that name #16816.&quot; Docker GitHub. November 10, 2016. Accessed December 17, 2016. https://github.com/docker/docker/issues/16816</li>
</ol>
<h3 id="software-virtualization---containers---lxc">Software Virtualization - Containers - LXC</h3>
<p>Linux Containers (LXC) utilizes the Linux kernel to natively run containers.</p>
<p>Debian install [1]:</p>
<pre><code># apt-get install lxc</code></pre>
<p>RHEL install [2] requires the Extra Packages for Enterprise Linux (EPEL) repository:</p>
<ul>
<li><p>RHEL:</p>
<pre><code># yum install epel-release
# yum install lxc lxc-templates libvirt</code></pre></li>
</ul>
<p>On RHEL family systems the <code>lxcbr0</code> interface is not created or used. Alternatively, the Libvirt interface <code>virbr0</code> should be uesd.</p>
<pre><code># vim /etc/lxc/default.conf
lxc.network.link = virbr0</code></pre>
<p>The required services need to be started before LXC containers will be able to run.</p>
<pre><code># systemctl start libvirtd
# systemctl start lxc</code></pre>
<p>Templates that can be referenced for LXC container creation can be found in the <code>/usr/share/lxc/templates/</code> directory.</p>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;LXC.&quot; Ubuntu Documentation. Accessed August 8, 2017. https://help.ubuntu.com/lts/serverguide/lxc.html</li>
<li>&quot;How to install and setup LXC (Linux Container) on Fedora Linux 26.&quot; nixCraft. July 13, 2017. Accessed August 8, 2017. https://www.cyberciti.biz/faq/how-to-install-and-setup-lxc-linux-container-on-fedora-linux-26/</li>
</ol>
<h1 id="orchestration">Orchestration</h1>
<p>Virtual machine provisioning can be automated through the use of different tools.</p>
<h2 id="orchestration---vagrant">Orchestration - Vagrant</h2>
<p>Vagrant is programmed in Ruby to help automate virtual machine (VM) deployment. It uses a single file called &quot;Vagrantfile&quot; to describe the virtual machines to create. By default, Vagrant will use VirtualBox as the hypervisor but other technologies can be used.</p>
<ul>
<li>Officially supported hypervisors [1]:
<ul>
<li>docker</li>
<li>hyperv</li>
<li>virtualbox</li>
<li>vmware_desktop</li>
<li>vmware_fusion</li>
</ul></li>
<li>Unofficial hypervisors [2]:
<ul>
<li>aws</li>
<li>azure</li>
<li>google</li>
<li>libvirt (KVM or Xen)</li>
<li>lxc</li>
<li>managed-servers (physical bare metal servers)</li>
<li>parallels</li>
<li>vsphere</li>
</ul></li>
</ul>
<p>Most unoffocial hypervisor providers can be automatically installed as a plugin from the command line.</p>
<pre><code>$ vagrant plugin install vagrant-&lt;HYPERVISOR&gt;</code></pre>
<p>Deploy VMs using a Vagrant file:</p>
<pre><code>$ vagrant up</code></pre>
<p>OR</p>
<pre><code>$ vagrant up --provider &lt;HYPERVISOR&gt;</code></pre>
<p>Destroy VMs using a Vagrant file:</p>
<pre><code>$ vagrant destroy</code></pre>
<p>The default username and password should be <code>vagrant</code>.</p>
<p>This guide can be followed for creating custom Vagrant boxes: https://www.vagrantup.com/docs/boxes/base.html.</p>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;Introduction to Vagrant.&quot; Vagrant Documentation. April 24, 2017. Accessed May 9, 2017. https://www.vagrantup.com/intro/getting-started/index.html</li>
<li>&quot;Available Vagrant Plugins.&quot; mitchell/vagrant GitHub. November 9, 2016. Accessed May 8, 2017. https://github.com/mitchellh/vagrant/wiki/Available-Vagrant-Plugins</li>
</ol>
<h3 id="orchestration---vagrant---vagrantfile">Orchestration - Vagrant - Vagrantfile</h3>
<p>A default Vagrantfile can be created to start customizing with.</p>
<pre><code>$ vagrant init</code></pre>
<p>All of the settings should be defined within the <code>Vagrant.configure()</code> block.</p>
<pre><code>Vagrant.configure(&quot;2&quot;) do |config|
    # Define VM settings here.
end</code></pre>
<p>Define the virtual machine template to use. This will be downloaded, by default (if the <code>box_url</code> is not changed) from the HashiCorp website.</p>
<ul>
<li>box = Required. The name of the virtual machine to download. A list of official virtual machines can be found at <code>https://atlas.hashicorp.com/boxes/search</code>.</li>
<li>box_version = The version of the virtual machine to use.</li>
<li>box_url = The URL to the virtual machine details.</li>
</ul>
<p>Example:</p>
<pre><code>Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.box = &quot;ubuntu/xenial64&quot;
  config.vm.box_version = &quot;v20170508.0.0&quot;
  config.vm.box_url = &quot;https://cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-vagrant.box&quot;
end</code></pre>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;[Vagrant] Boxes.&quot; Vagrant Documentation. April 24, 2017. Accessed May 9, 2017. https://www.vagrantup.com/docs/boxes.html</li>
</ol>
<h4 id="orchestration---vagrant---vagrantfile---networks">Orchestration - Vagrant - Vagrantfile - Networks</h4>
<p>Networks are either <code>private</code> or <code>public</code>. <code>private</code> networks use host-only networking and use network address translation (NAT) to communicate out to the Internet. Virtual machines (VMs) can communicate with each other but they cannot be reached from the outside world. Port forwarding can also be configured to allow access to specific ports from the hypervisor node. <code>public</code> networks allow a virtual machine to attach to a bridge device for full connectivity with the external network. This section covers VirtualBox networks since it is the default virtualization provider.</p>
<p>With a <code>private</code> network, the IP address can either be a random address assigned by DHCP or a static IP that is defined.</p>
<pre><code>Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.network &quot;private_network&quot;, type: &quot;dhcp&quot;
end</code></pre>
<pre><code>Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.network &quot;private_network&quot;, ip: &lt;IP4_OR_IP6_ADDRESS&gt;, netmask: &lt;SUBNET_MASK&gt;
end</code></pre>
<p>The same rules apply to <code>public</code> networks except it uses the external DHCP server on the network (if it exists).</p>
<pre><code>Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.network &quot;public_network&quot;, use_dhcp_assigned_default_route: true
end</code></pre>
<p>When a <code>public</code> network is defined and no interface is given, the end-user is prompted to pick a physical network interface device to bridge onto for public network access. This bridge device can also be specified manually.</p>
<pre><code>Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.network &quot;public_network&quot;, bridge: &quot;eth0: First NIC&quot;
end</code></pre>
<p>In this example, port 2222 on the localhost (127.0.0.1) of the hypervisor will forward to port 22 of the VM.</p>
<pre><code>...
    config.vm.network &quot;forwarded_port&quot;, id: &quot;ssh&quot;, guest: 22, host: 2222
...</code></pre>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;[Vagrant] Networking.&quot; Vagrant Documentation. April 24, 2017. Accessed May 9, 2017. https://www.vagrantup.com/docs/networking/</li>
</ol>
<h5 id="orchestration---vagrant---vagrantfile---networks---libvirt">Orchestration - Vagrant - Vagrantfile - Networks - Libvirt</h5>
<p>The options and syntax for public networks with the &quot;libvirt&quot; provider are slightly different.</p>
<p>Options:</p>
<ul>
<li>dev = The bridge device name.</li>
<li>mode = The libvirt mode to use. Default: <code>bridge</code>.</li>
<li>type = The libvirt interface type. This is normally set to <code>bridge</code>.</li>
<li>network_name = The name of a network to use.</li>
<li>portgroup = The libvirt portgroup to use.</li>
<li>ovs = Instead of using a Linux bridge, use Open vSwitch instead. Default: <code>false</code>.</li>
<li>trust_guest_rx_filters = Enable the <code>trustGuestRxFilters</code> setting. Default: <code>false</code>.</li>
</ul>
<p>Example:</p>
<pre><code>config.vm.define &quot;controller&quot; do |controller|
    controller.vm.network &quot;public_network&quot;, ip: &quot;10.0.0.205&quot;, dev: &quot;br0&quot;, mode: &quot;bridge&quot;, type: &quot;bridge&quot;
end</code></pre>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Vagrant Libvirt Provider [README].&quot; vagrant-libvirt GitHub. May 8, 2017. Accessed June 17, 2017. https://github.com/vagrant-libvirt/vagrant-libvirt</li>
</ol>
<h4 id="orchestration---vagrant---vagrantfile---provisioning">Orchestration - Vagrant - Vagrantfile - Provisioning</h4>
<p>After a virtual machine (VM) has been created, additional commands can be run to configure the guest VMs. This is referred to as &quot;provisioning.&quot;</p>
<ul>
<li>Provisioners [1]:
<ul>
<li><a href="https://www.vagrantup.com/docs/provisioning/ansible_intro.html">ansible</a> = Run a Ansible Playbook from the hypervisor node.</li>
<li>ansible_local = Run a Ansible Playbook from within the VM.</li>
<li>cfengine = Use CFEngine to configure the VM.</li>
<li>chef_solo = Run a Chef Cookbook from inside the VM using <code>chef-solo</code>.</li>
<li>chef_zero = Run a Chef Cookbook, but uset <code>chef-zero</code> to emulate a Chef server inside of the VM.</li>
<li>chef_client = Use a remote Chef server to run a Cookbook inside the VM.</li>
<li>chef_apply = Run a Chef recipe with <code>chef-apply</code>.</li>
<li>docker = Install and configure Docker inside of the VM.</li>
<li>file = Copy files from the hypervisor to the VM. Note that the directory that the <code>Vagrantfile</code> is in will be mounted as the directory <code>/vagrant/</code> inside of the VM.</li>
<li>puppet = Run single Puppet manifests with <code>puppet apply</code>.</li>
<li>puppet_server = Run a Puppet manifest inside of the VM using an external Puppet server.</li>
<li>salt = Run Salt states inside of the VM.</li>
<li>shell = Run CLI shell commands.</li>
</ul></li>
</ul>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;[Vagrant] Provisioning.&quot; Vagrant Documentation. April 24, 2017. Accessed May 9, 2017. https://www.vagrantup.com/docs/provisioning/</li>
</ol>
<h4 id="orchestration---vagrant---vagrantfile---multiple-machines">Orchestration - Vagrant - Vagrantfile - Multiple Machines</h4>
<p>A <code>Vagrantfile</code> can specify more than one virtual machine.</p>
<p>The recommended way to provision multiple VMs is to statically define each individual VM to create as shown here. [1]</p>
<pre><code>Vagrant.configure(&quot;2&quot;) do |config|

  config.vm.define &quot;web&quot; do |web|
    web.vm.box = &quot;nginx&quot;
  end

  config.vm.define &quot;php&quot; do |php|
    php.vm.box = &quot;phpfpm&quot;
  end

  config.vm.define &quot;db&quot; do |db|
    db.vm.box = &quot;mariadb&quot;
  end

end</code></pre>
<p>However, it is possible to use Ruby to dynamically define and create VMs. This will work for creating the VMs but using the <code>vagrant</code> command to manage the VMs will not work properly [2]:</p>
<pre><code>servers=[
  {
    :hostname =&gt; &quot;web&quot;,
    :ip =&gt; &quot;10.0.0.10&quot;,
    :box =&gt; &quot;xenial&quot;,
    :ram =&gt; 1024,
    :cpu =&gt; 2
  },
  {
    :hostname =&gt; &quot;db&quot;,
    :ip =&gt; &quot;10.10.10.11&quot;,
    :box =&gt; &quot;saucy&quot;,
    :ram =&gt; xenial,
    :cpu =&gt; 4
  }
]

Vagrant.configure(2) do |config|
    servers.each do |machine|
        config.vm.define machine[:hostname] do |node|
            node.vm.box = machine[:box]
            node.vm.hostname = machine[:hostname]
            node.vm.network &quot;private_network&quot;, ip: machine[:ip]
            node.vm.provider &quot;virtualbox&quot; do |vb|
                vb.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, machine[:ram]]
            end
        end
    end
end</code></pre>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;[Vagrant] Multi-Machine.&quot; Vagrant Documentation. April 24, 2017. Accessed May 9, 2017. https://www.vagrantup.com/docs/multi-machine/</li>
<li>&quot;Vagrantfile.&quot; Linux system administration and monitoring / Windows servers and CDN video. May 9, 2017. Accessed May 9, 2017. http://sysadm.pp.ua/linux/sistemy-virtualizacii/vagrantfile.html</li>
</ol>
