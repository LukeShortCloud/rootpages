<h1 id="openstack">OpenStack</h1>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#overview">Overview</a></li>
<li><a href="#installation">Installation</a>
<ul>
<li><a href="#installation---packstack">PackStack</a></li>
<li><a href="#installation---openstack-ansible">OpenStack Ansible</a></li>
<li><a href="#installation---tripleo">TripleO</a>
<ul>
<li><a href="#installation---tripleo---quick">Quick</a></li>
<li><a href="#installation---tripleo---full">Full</a></li>
</ul></li>
</ul></li>
<li><a href="#configurations">Configurations</a>
<ul>
<li><a href="#configurations---common">Common</a>
<ul>
<li><a href="#configurations---common---database">Database</a></li>
</ul></li>
<li><a href="#configurations---keystone">Keystone</a>
<ul>
<li><a href="#configurations---keystone---token-provider">Token Provider</a></li>
<li><a href="#configurations---keystone---api-v3">API v3</a></li>
</ul></li>
<li><a href="#configurations---nova">Nova</a>
<ul>
<li><a href="#configurations---nova---hypervisors">Hypervisors</a></li>
<li><a href="#configurations---nova---cpu-pinning">CPU Pinning</a></li>
</ul></li>
<li><a href="#configurations---neutron">Neutron</a>
<ul>
<li><a href="#configurations---neutron---dns">DNS</a></li>
<li><a href="#configurations---neutron---metadata">Metadata</a></li>
<li><a href="#configurations---neutron---load-balancing-as-a-service">Load-Balancing-as-a-Service</a></li>
<li><a href="#configurations---neutron---quality-of-service">Quality of Service</a></li>
</ul></li>
<li><a href="#configurations---cinder">Cinder</a>
<ul>
<li><a href="#configurations---cinder---ceph">Ceph</a></li>
</ul></li>
</ul></li>
<li><a href="#upgrades">Upgrades</a></li>
<li><a href="#command-line-interface-utilities">Command Line Interface Utilities</a></li>
<li><a href="#automation">Automation</a>
<ul>
<li><a href="#automation---heat">Heat</a>
<ul>
<li><a href="#automation---heat---resources">Resources</a></li>
<li><a href="#automation---heat---parameters">Parameters</a></li>
</ul></li>
<li><a href="#automation---vagrant">Vagrant</a></li>
</ul></li>
<li><a href="#testing">Testing</a>
<ul>
<li><a href="#testing---tempest">Tempest</a></li>
</ul></li>
<li><a href="#performance">Performance</a></li>
</ul>
<h1 id="introduction">Introduction</h1>
<p>This guide is aimed to help guide System Administrators through OpenStack. It is assumed that the cloud is using these industry standard software:</p>
<ul>
<li>OpenStack Mitaka</li>
<li>CentOS 7 (Linux)</li>
</ul>
<p>Most topics mentioned in this guide can be applied to similar environments.</p>
<h1 id="overview">Overview</h1>
<p>OpenStack has a large range of services that manage different different components in a modular way.</p>
<p>Core services:</p>
<ul>
<li>Keystone = Authentication</li>
<li>Nova = Compute</li>
<li>Neutron = Networking</li>
</ul>
<p>Extra services:</p>
<ul>
<li>Horizon = Dashboard</li>
<li>Swift = Object Storage</li>
<li>Cinder = Block Storage</li>
<li>Glance = Image</li>
<li>Ceilometer = Telemtry</li>
<li>Heat = Orchestration</li>
<li>Trove = Database</li>
<li>Sahara = Elastic Map Reduce</li>
<li>Ironic = Bare-Metal Provisioning</li>
<li>Zaqar = Messaging Service</li>
<li>Manila = Shared Filesystems</li>
<li>Designate = DNS Service</li>
<li>Barbican = Key Management</li>
<li>Magnum = Containers</li>
<li>Murano = Application Catalog</li>
<li>Congress = Governance</li>
</ul>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Project Navigator.&quot; OpenStack. Accessed January 15, 2017. https://www.openstack.org/software/project-navigator/</li>
</ol>
<h1 id="installation">Installation</h1>
<p>It is possible to easily install OpenStack all-in-one (AIO) server. This provides a means to quickly and easily test changes and updates to configuration files and code. This is ideal for developers and System Administrators looking for a proof of concept.</p>
<h2 id="installation---packstack">Installation - PackStack</h2>
<p>Supported operating systems: RHEL 7</p>
<p>PackStack provides a simple all-in-one development. This is not meant for production but works well for developers needing to test new features.</p>
<pre><code># yum install https://repos.fedorapeople.org/repos/openstack/openstack-mitaka/rdo-release-mitaka-6.noarch.rpm
# yum install packstack
# packstack --gen-answer-file &lt;FLIE&gt;
# packstack --answer-file &lt;FILE&gt;</code></pre>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;OpenStack packages.&quot; OpenStack Installation Guide for Red Hat Enterprise Linux and CentOS. http://docs.openstack.org/mitaka/install-guide-rdo/environment-packages.html</li>
</ol>
<h2 id="installation---openstack-ansible">Installation - OpenStack Ansible</h2>
<p>Supported operating systems: Ubuntu 14.04 or 16.04</p>
<p>OpenStack Ansible uses Ansible for automating the deployment of Docker containers that run the OpenStack services. This was created by RackSpace as an official tool for deploying and managing production environments.</p>
<pre><code># apt-get install git
# git clone https://git.openstack.org/openstack/openstack-ansible /opt/openstack-ansible
# cd /opt/openstack-ansible/
# git checkout 13.3.7
# scripts/bootstrap-ansible.sh
# scripts/bootstrap-aio.sh</code></pre>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Quick Start.&quot; OpenStack Ansible Developer Documentation. January 10, 2016. Accessed January 10, 2016. http://docs.openstack.org/developer/openstack-ansible/developer-docs/quickstart-aio.html</li>
</ol>
<h2 id="installation---tripleo">Installation - TripleO</h2>
<p>Supported operating systems: RHEL 7, Fedora &gt;= 22</p>
<p>TripleO means &quot;OpenStack on OpenStack.&quot; The Undercloud is first deployed in a small, usually all-in-one, environment. This server is then used to create and manage a full Overcloud cluster. Virtual machines or physical servers can be used. The minimum requirement of RAM for the host node is 16GB to run all of the OpenStack services. [1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;tripleo-quickstart.&quot; TripleO Quickstart GitHub. January 10, 2017. Accessed January 15, 2017. https://github.com/openstack/tripleo-quickstart</li>
</ol>
<h3 id="installation---tripleo---quick">Installation - TripleO - Quick</h3>
<p>The &quot;TripleO-Quickstart&quot; project was created to use Ansible to automate deploying TripleO as fast as possible.</p>
<pre><code>$ curl -O https://raw.githubusercontent.com/openstack/tripleo-quickstart/master/quickstart.sh
$ bash quickstart.sh --release mitaka &lt;VIRTUAL_MACHINE_IP&gt;</code></pre>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;TripleO quickstart.&quot; RDO Project. Accessed January 8, 2016. https://www.rdoproject.org/tripleo/</li>
</ol>
<h3 id="installation---tripleo---full">Installation - TripleO - Full</h3>
<ul>
<li><p>Install the EPEL for extra packages that will be required.</p>
<pre><code># yum install epel-release</code></pre></li>
<li><p>Install the Undercloud environment deployment tools.</p>
<pre><code># yum install instack-undercloud</code></pre></li>
<li><p>Deploy the Undercloud virtual machine.</p>
<pre><code># instack–virt–setup</code></pre></li>
<li><p>Log into the virtual machine with the provided credentials from the previous command.</p>
<pre><code># ssh root@&lt;VIRTUAL_MACHINE_IP&gt;</code></pre></li>
<li><p>Install TripleO from the RDO Delorean repository.</p>
<pre><code># yum install python-tripleoclient</code></pre></li>
<li><p>Deploy an all-in-one Undercloud on the virtual machine.</p>
<pre><code># openstack undercloud install</code></pre></li>
</ul>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;How To Install OpenStack Using TripleO.&quot; Platform9 Blog. June 27, 2016. Accessed January 8, 2017. https://platform9.com/blog/install-openstack-using-tripleo/</li>
</ol>
<h1 id="configurations">Configurations</h1>
<p>This section will focus on important settings for each service's configuration files.</p>
<h2 id="configurations---common">Configurations - Common</h2>
<p>These are general configuration options that apply to most OpenStack configuration files.</p>
<h3 id="configurations---common---database">Configurations - Common - Database</h3>
<p>Different database backends can be used by the API services on the controller nodes.</p>
<ul>
<li><p>MariaDB/MySQL. Requires the &quot;PyMySQL&quot; Python library. Starting with Liberty, this is prefered over using &quot;mysql://&quot; as the latest OpenStack libraries are written for PyMySQL connections (not to be confused with &quot;MySQL-python&quot;). [1]</p>
<pre><code>[ database ] connection = mysql+pymysql://&lt;USER&gt;:&lt;PASSWORD&gt;@&lt;MYSQL_HOST&gt;/&lt;DATABASE&gt;</code></pre></li>
<li><p>PostgreSQL. Requires the &quot;psycopg2&quot; Python library. [2]</p>
<pre><code>[ database ] connection = postgresql://&lt;USER&gt;:&lt;PASSWORD&gt;@&lt;MYSQL_HOST&gt;/&lt;DATABASE&gt;</code></pre></li>
<li><p>SQLite.</p>
<pre><code>[ database ] connection = sqlite:///&lt;DATABASE&gt;.sqlite</code></pre></li>
</ul>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;DevStack switching from MySQL-python to PyMySQL.&quot; OpenStack nimeyo. Jun 9, 2015. Accessed October 15, 2016. https://openstack.nimeyo.com/48230/openstack-all-devstack-switching-from-mysql-python-pymysql</li>
<li>&quot;Using PostgreSQL with OpenStack.&quot; FREE AND OPEN SOURCE SOFTWARE KNOWLEDGE BASE. June 06, 2014. Accessed October 15, 2016. https://fosskb.in/2014/06/06/using-postgresql-with-openstack/</li>
</ol>
<h2 id="configurations---keystone">Configurations - Keystone</h2>
<h3 id="configurations---keystone---api-v3">Configurations - Keystone - API v3</h3>
<p>In Mitaka, the Keystone v2.0 API has been deprecated. It will be removed entirely from OpenStack in the &quot;Q&quot; release. [1] It is possible to run both v2.0 and v3 at the same time but it's desirable to move towards the v3 standard. If both have to be enabled, services should be configured to use v2.0 or else problems can occur with v3's domain scoping. For disabling v2.0 entirely, Keystone's API paste configuration needs to have these lines removed (or commented out) and then the web server should be restarted.</p>
<ul>
<li>/etc/keystone/keystone-paste.ini
<ul>
<li>[pipeline:public_api] pipeline = cors sizelimit url_normalize request_id admin_token_auth build_auth_context token_auth json_body ec2_extension public_service</li>
<li>[pipeline:admin_api] pipeline = cors sizelimit url_normalize request_id admin_token_auth build_auth_context token_auth json_body ec2_extension s3_extension admin_service</li>
<li>[composite:main] /v2.0 = public_api</li>
<li>[composite:admin] /v2.0 = admin_api</li>
</ul></li>
</ul>
<p>[2]</p>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;Mitaka Series Release Notes.&quot; Accessed October 16, 2016. http://docs.openstack.org/releasenotes/keystone/mitaka.html</li>
<li>&quot;Setting up an RDO deployment to be Identity V3 Only.&quot; Young Logic. May 8, 2015. Accessed October 16, 2016. https://adam.younglogic.com/2015/05/rdo-v3-only/</li>
</ol>
<h3 id="configurations---keystone---token-provider">Configurations - Keystone - Token Provider</h3>
<p>The token provider is used to create and delete tokens for authentication. Different providers can be used as the backend.</p>
<h4 id="scenario-1---uuid-default">Scenario #1 - UUID (default)</h4>
<ul>
<li>/etc/keystone/keystone.conf
<ul>
<li>[ token ] provider = uuid</li>
</ul></li>
</ul>
<h4 id="scenario-2---pki">Scenario #2 - PKI</h4>
<p>PKI tokens are deprecated and will be removed in the Ocata release. [3]</p>
<ul>
<li>/etc/keystone/keystone.conf
<ul>
<li>[ token ] provider = pki</li>
</ul></li>
<li><p>Create the certificates. A new directory &quot;/etc/keystone/ssl/&quot; will be used to store these files.</p>
<pre><code># keystone-manage pki_setup --keystone-user keystone --keystone-group keystone</code></pre></li>
</ul>
<h4 id="scenario-3---fernet-fastest-token-creation">Scenario #3 - Fernet (fastest token creation)</h4>
<ul>
<li>/etc/keystone/keystone.conf
<ul>
<li>[ token ] provider = fernet</li>
<li>[ fernet_tokens ] key_repository = /etc/keystone/fernet-keys/</li>
</ul></li>
<li><p>Create the Fernet keys:</p>
<pre><code># mkdir /etc/keystone/fernet-keys/
# chmod 750 /etc/keystone/fernet-keys/
# keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone</code></pre></li>
</ul>
<p>[2]</p>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;Configuring Keystone.&quot; OpenStack Documentation. Accessed October 16, 2016. http://docs.openstack.org/developer/keystone/configuration.html</li>
<li>&quot;OpenStack Keystone Fernet tokens.&quot; Dolph Mathews. Accessed August 27th, 2016. http://dolphm.com/openstack-keystone-fernet-tokens/</li>
<li>&quot;Newton Series Release Notes.&quot; OpenStack Documentation. Accessed January 15, 2016. http://docs.openstack.org/releasenotes/keystone/newton.html</li>
</ol>
<h2 id="configurations---nova">Configurations - Nova</h2>
<ul>
<li>/etc/nova/nova.conf
<ul>
<li>[ libvirt ] inject_key = false
<ul>
<li>Do not inject SSH keys via Nova. This should be handled by the Nova's metadata service. This will either be &quot;openstack-nova-api&quot; or &quot;openstack-nova-metadata-api&quot; depending on your setup.</li>
</ul></li>
<li>[ DEFAULT ] enabled_apis = osapi_compute,metadata
<ul>
<li>Enable support for the Nova API, and Nova's metadata API. If &quot;metedata&quot; is specified here, then the &quot;openstack-nova-api&quot; handles the metadata and not &quot;openstack-nova-metadata-api.&quot;</li>
</ul></li>
<li>[ api_database ] connection = connection=mysql://nova:password@10.1.1.1/nova_api</li>
<li>[ database ] connection = mysql://nova:password@10.1.1.1/nova
<ul>
<li>For the controller nodes, specify the connection SQL connection string. In this example it uses MySQL, the MySQL user &quot;nova&quot; with a password called &quot;password&quot;, it connects to the IP address &quot;10.1.1.1&quot; and it is using the database &quot;nova.&quot;</li>
</ul></li>
</ul></li>
</ul>
<h3 id="configurations---nova---hypervisors">Configurations - Nova - Hypervisors</h3>
<p>Nova supports a wide range of virtualization technologies. Full hardware virtulization, paravirutlization, or containers can be used. Even Windows' Hyper-V is supported. [1]</p>
<h4 id="scenario-1---kvm">Scenario #1 - KVM</h4>
<ul>
<li>/etc/nova/nova.conf
<ul>
<li>[DEFAULT] compute_driver = libvirt.LibvirtDriver</li>
<li>[libvirt] virt_type = kvm</li>
</ul></li>
</ul>
<p>[2]</p>
<h4 id="scenario-2---xen">Scenario #2 - Xen</h4>
<ul>
<li>/etc/nova/nova.conf
<ul>
<li>[DEFAULT] compute_driver = libvirt.LibvirtDriver</li>
<li>[libvirt] virt_type = xen</li>
</ul></li>
</ul>
<p>[3]</p>
<h4 id="scenario-3---lxc">Scenario #3 - LXC</h4>
<ul>
<li>/etc/nova/nova.conf
<ul>
<li>[DEFAULT] compute_driver = libvirt.LibvirtDriver</li>
<li>[libvirt] virt_type = lxc</li>
</ul></li>
</ul>
<p>[4]</p>
<h4 id="scenario-4---docker">Scenario #4 - Docker</h4>
<p>Docker is not available by default in OpenStack. First it must be installed before configuring it in Nova.</p>
<pre><code># git clone https://github.com/openstack/nova-docker.git
# cd nova-docker/
# pip install -r requirements.txt
# python setup.py install</code></pre>
<ul>
<li>[DEFAULT] compute_driver = novadocker.virt.docker.DockerDriver</li>
</ul>
<p>[5]</p>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>&quot;Hypervisors.&quot; OpenStack Configuration Reference - Liberty. Accessed August 28th, 2016. http://docs.openstack.org/liberty/config-reference/content/section_compute-hypervisors.html</li>
<li>&quot;KVM.&quot; OpenStack Configuration Reference - Liberty. Accessed August 28th, 2016. http://docs.openstack.org/liberty/config-reference/content/kvm.html</li>
<li>&quot;Xen.&quot; OpenStack Configuration Reference - Liberty. Accessed August 28th, 2016. http://docs.openstack.org/liberty/config-reference/content/xen_libvirt.html</li>
<li>&quot;LXC.&quot; OpenStack Configuration Reference - Liberty. Accessed August 28th, 2016. http://docs.openstack.org/liberty/config-reference/content/lxc.html</li>
<li>&quot;nova-docker.&quot; GitHub.com. December, 2015. Accessed August 28th, 2016. https://github.com/openstack/nova-docker</li>
</ol>
<h3 id="configurations---nova---cpu-pinning">Configurations - Nova - CPU Pinning</h3>
<p>Verify that the processor(s) has hardware support for non-uniform memory access (NUMA). If it does, NUMA may still need to be turned on in the BIOS. NUMA nodes are the physical processors. These processors are then mapped to specific sectors of RAM. [1]</p>
<pre><code># lscpu | grep NUMA
NUMA node(s):          2
NUMA node0 CPU(s):     0-9,20-29
NUMA node1 CPU(s):     10-19,30-39</code></pre>
<pre><code># numactl --hardware
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 20 21 22 23 24 25 26 27 28 29
node 0 size: 49046 MB
node 0 free: 31090 MB
node 1 cpus: 10 11 12 13 14 15 16 17 18 19 30 31 32 33 34 35 36 37 38 39
node 1 size: 49152 MB
node 1 free: 31066 MB
node distances:
node   0   1
  0:  10  21
  1:  21  10</code></pre>
<pre><code># virsh nodeinfo | grep NUMA
NUMA cell(s):        2</code></pre>
<p>[1][3]</p>
<h4 id="configuration">Configuration</h4>
<ul>
<li><p>Append the two NUMA filters.</p>
<pre><code># vim /etc/nova/nova.conf
[ DEFAULT ] scheduler_default_filters = RetryFilter,AvailabilityZoneFilter,RamFilter,DiskFilter,ComputeFilter,ComputeCapabilitiesFilter,ImageProp
ertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter,NUMATopologyFilter,AggregateInstanceExtraSpecsFilter</code></pre></li>
<li><p>Restart the Nova scheduler service on the controller node(s).</p>
<pre><code># systemctl restart openstack-nova-scheduler</code></pre></li>
<li><p>Set the aggregate/availability zone to allow pinning.</p>
<pre><code># nova aggregate-set-metadata &lt;AGGREGATE_ZONE&gt; pinned=true</code></pre></li>
<li><p>Modify a flavor to provide dedicated CPU pinning.</p>
<pre><code># nova flavor-key &lt;FLAVOR_ID&gt; set hw:cpu_policy=dedicated</code></pre></li>
</ul>
<p>[1][2][3]</p>
<p>Sources:</p>
<ol style="list-style-type: decimal">
<li>http://redhatstackblog.redhat.com/2015/05/05/cpu-pinning-and-numa-topology-awareness-in-openstack-compute/</li>
<li>http://docs.openstack.org/admin-guide/compute-flavors.html</li>
<li>http://www.stratoscale.com/blog/openstack/cpu-pinning-and-numa-awareness/</li>
</ol>
<h2 id="configurations---neutron">Configurations - Neutron</h2>
<h3 id="configurations---neutron---dns">Configurations - Neutron - DNS</h3>
<p>By default, Neutron does not provide any DNS resolvers. This means that DNS will not work. It is possible to either provide a default list of name servers or configure Neutron to refer to the relevant /etc/resolv.conf file on the server.</p>
<h4 id="scenario-1---define-default-resolvers-recommended">Scenario #1 - Define default resolvers (recommended)</h4>
<ul>
<li>/etc/neutron/dhcp_agent.ini
<ul>
<li>[ DEFAULT ] dnsmasq_dns_servers = 8.8.8.8,8.8.4.4</li>
</ul></li>
</ul>
<h4 id="scenario-2---leave-resolvers-to-be-configured-by-the-subnet-details">Scenario #2 - Leave resolvers to be configured by the subnet details</h4>
<ul>
<li>Nothing needs to be configured.</li>
</ul>
<h4 id="scenario-3---do-not-provide-resolvers">Scenario #3 - Do not provide resolvers</h4>
<ul>
<li>/etc/neutron/dhcp_agent.ini
<ul>
<li>[ DEFAULT ] dnsmasq_local_resolv = True</li>
</ul></li>
</ul>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Name resolution for instances.&quot; OpenStack Documentation. August 09, 2016. Accessed August 13th, 2016. http://docs.openstack.org/mitaka/networking-guide/config-dns-resolution.html</li>
</ol>
<h3 id="configurations---neutron---metadata">Configurations - Neutron - Metadata</h3>
<p>The metadata service provides useful information about the instance from the IP address 169.254.169.254/32. This service is also used to communicate with &quot;cloud-init&quot; on the instance to configure SSH keys and other post-boot tasks.</p>
<p>Assuming authentication is already configured, set these options for the OpenStack environment. These are the basics needed before the metadata service can be used correctly. Then you can choose to use DHCP namespaces (layer 2) or router namespaces (layer 3) for delievering/recieving requests.</p>
<ul>
<li>/etc/neutron/metadata_agent.ini
<ul>
<li>[ DEFAULT ] nova_metadata_ip = CONTROLLER_IP</li>
<li>[ DEFAULT ] metadata_proxy_shared_secret = SECRET_KEY</li>
</ul></li>
<li>/etc/nova/nova.conf
<ul>
<li>[ DEFAULT ] enabled_apis = osapi_compute,metadata</li>
<li>[ neutron ] service_metadata_proxy = True</li>
<li>[ neutron ] metadata_proxy_shared_secret = SECRET_KEY</li>
</ul></li>
</ul>
<h4 id="scenario-1---dhcp-namespace-recommended-for-dvr">Scenario #1 - DHCP Namespace (recommended for DVR)</h4>
<ul>
<li>/etc/neutron/dhcp_agent.ini
<ul>
<li>[ DEFAULT ] force_metadata = True</li>
<li>[ DEFAULT ] enable_isolated_metadata = True</li>
<li>[ DEFAULT ] enable_metadata_network = True</li>
</ul></li>
<li>/etc/neutron
<ul>
<li>[ DEFAULT ] enable_metadata_proxy = False</li>
</ul></li>
</ul>
<h4 id="scenario-2---router-namespace">Scenario #2 - Router Namespace</h4>
<ul>
<li>/etc/neutron/dhcp_agent.ini
<ul>
<li>[ DEFAULT ] force_metadata = False</li>
<li>[ DEFAULT ] enable_isolated_metadata = True</li>
<li>[ DEFAULT ] enable_metadata_network = False</li>
</ul></li>
<li>/etc/neutron/l3_agent.ini
<ul>
<li>[ DEFAULT ] enable_metadata_proxy = True</li>
</ul></li>
</ul>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Introduction of Metadata Service in OpenStack.&quot; VietStack. September 09, 2014. Accessed August 13th, 2016. https://vietstack.wordpress.com/2014/09/27/introduction-of-metadata-service-in-openstack/</li>
</ol>
<h3 id="configurations---neutron---load-balancing-as-a-service">Configurations - Neutron - Load-Balancing-as-a-Service</h3>
<p>Load-Balancing-as-a-Service version 2 (LBaaSv2) has been stable since Liberty. It can be configured with either the HAProxy or Octavia back-end.</p>
<ul>
<li>/etc/neutron/neutron.conf
<ul>
<li>[ DEFAULT ] service_plugins = <code>&lt;PLUGINS&gt;</code>, neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2
<ul>
<li>Append the LBaaSv2 service plugin.</li>
</ul></li>
</ul></li>
<li>/etc/neutron/lbaas_agent.ini
<ul>
<li>[ DEFAULT ] interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver
<ul>
<li>This is for Neutron with the OpenVSwitch backend only.</li>
</ul></li>
<li>[ DEFAULT ] interface_driver = neutron.agent.linux.interface.BridgeInterfaceDriver
<ul>
<li>This is for Neutron with the Linux Bridge backend only.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="scenario-1---haproxy-recommended-for-its-maturity">Scenario #1 - HAProxy (recommended for it's maturity)</h4>
<ul>
<li>/etc/neutron/neutron_lbaas.conf
<ul>
<li>[ service_providers ] service_provider = LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default</li>
</ul></li>
<li>/etc/neutron/lbaas_agent.ini
<ul>
<li>[ DEFAULT ] device_driver = neutron_lbaas.drivers.haproxy.namespace_driver.HaproxyNSDriver</li>
<li>[ haproxy ] user_group = haproxy
<ul>
<li>Specify the group that HAProxy runs as. In RHEL, it's &quot;haproxy.&quot;</li>
</ul></li>
</ul></li>
</ul>
<h4 id="scenario-2---octavia">Scenario #2 - Octavia</h4>
<ul>
<li>/etc/neutron/neutron_lbaas.conf
<ul>
<li>[ service_providers ] service_provider = LOADBALANCERV2:Octavia:neutron_lbaas.drivers.octavia.driver.OctaviaDriver:default</li>
</ul></li>
</ul>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>http://docs.openstack.org/draft/networking-guide/config-lbaas.html</li>
</ol>
<h3 id="configurations---neutron---quality-of-service">Configurations - Neutron - Quality of Service</h3>
<p>The Quality of Service (QoS) plugin can be used to rate limit the amount of bandwidth that is allowed through a network port.</p>
<ul>
<li>/etc/neutron/neutron.conf
<ul>
<li>[ DEFAULT ] service_plugins = neutron.services.qos.qos_plugin.QoSPlugin
<ul>
<li>Append the QoS plugin to the list of service_plugins.</li>
</ul></li>
</ul></li>
<li>/etc/neutron/plugins/ml2/openvswitch_agent.ini
<ul>
<li>[ml2] extension_drivers = qos
<ul>
<li>Append the QoS driver with the modular layer 2 plugin provider. Alternatively to OpenVSwitch, LinuxBridge and SR-IOV are also supported for quality of service.</li>
</ul></li>
</ul></li>
<li>/etc/neutron/plugins/ml2/ml2_conf.ini
<ul>
<li>[agent] extensions = qos
<ul>
<li>Lastly, append the QoS extension to the moduler layer 2 configuration.</li>
</ul></li>
</ul></li>
</ul>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Quality of Service (QoS).&quot; OpenStack Documentation. October 10, 2016. Accessed October 16, 2016. http://docs.openstack.org/draft/networking-guide/config-qos.html</li>
</ol>
<h2 id="configurations---cinder">Configurations - Cinder</h2>
<p>The Cinder service provides block devices for instances.</p>
<h3 id="configurations---cinder---ceph">Configurations - Cinder - Ceph</h3>
<p>Ceph has become the most popular backend to Cinder due to it's high availability and scalability.</p>
<ul>
<li>/etc/cinder/cinder.conf
<ul>
<li>[ DEFAULT ] enabled_backends = ceph
<ul>
<li>Use the &quot;[ ceph ]&quot; section for the backend configuration. The new &quot;[ ceph ]&quot; section can be named anything but the same name must be used here.</li>
</ul></li>
<li>[ DEFAULT ] volume_backend_name = volumes</li>
<li>[ DEFAULT ] rados_connect_timeout = -1</li>
<li>[ ceph ] volume_driver = cinder.volume.drivers.rbd.RBDDriver
<ul>
<li>Use Cinder's RBD Python library.</li>
</ul></li>
<li>[ ceph ] rbd_pool = volumes
<ul>
<li>This is the RBD pool to use for volumes.</li>
</ul></li>
<li>[ ceph ] rbd_ceph_conf = /etc/ceph/ceph.conf</li>
<li>[ ceph ] rbd_flatten_volume_from_snapshot = false
<ul>
<li>Ceph supports efficent thin provisioned snapshots.</li>
</ul></li>
<li>[ ceph ] rbd_max_clone_depth = 5</li>
<li>[ ceph ] rbd_store_chunk_size = 4</li>
<li>[ ceph ] rados_connect_timeout = -1</li>
<li>[ ceph ] glance_api_version = 2</li>
</ul></li>
</ul>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;BLOCK DEVICES AND OPENSTACK.&quot; Ceph Documentation. http://docs.ceph.com/docs/master/rbd/rbd-openstack</li>
</ol>
<h1 id="upgrades">Upgrades</h1>
<p>Upgrading a produciton OpenStack environment requires a lot of planning. It is recommended to test an upgrade of the environment virtually before rolling it out to production. Automation tools generally have their own guides but most of these guidelines should still apply to manual deployment upgrades. The entire steps include to:</p>
<ul>
<li>Backup configuration files and databases.</li>
<li>Review the release notes of the OpenStack services that will be upgraded. These will contain details of deprecations and new configuration changes. <a href="https://releases.openstack.org/" class="uri">https://releases.openstack.org/</a></li>
<li>Update configuration files. Sample configurations can be found at <code>http://docs.openstack.org/&lt;RELEASE&gt;/config-reference/</code>.</li>
<li>If not already, consider using an automation tool such as Ansible to deploy new service configurations.</li>
<li>Remove the old package repository for OpenStack.</li>
<li>Add the new package repository for OpenStack.</li>
<li>Update all of the packages.</li>
<li>Restart the services. <code>openstack-service restart</code></li>
</ul>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Upgrades.&quot; OpenStack Documentation. January 15, 2017. Accessed January 15, 2017. http://docs.openstack.org/ops-guide/ops-upgrades.html</li>
</ol>
<h1 id="command-line-interface-utilities">Command Line Interface Utilities</h1>
<p>The OpenStack CLI resources used to be handled by seperate commands. These have all been modified and are managed by the universal &quot;openstack&quot; command. The various options and arguments are explained in Root Pages' OpenStack section <a href="https://raw.githubusercontent.com/ekultails/rootpages/master/linux_commands.xlsx">Linux Commands excel sheet</a>.</p>
<p>For the CLI utilies to work, the environment variables need to be set for the project and user. This way the commands can automatically authenticate.</p>
<ul>
<li>Add the credentials to a text file This is generally ends with the &quot;.sh&quot; extension to signify it's a shell file. A few default variables are filled in below.</li>
<li><p>Keystone v2.0</p>
<pre><code># unset any variables used
unset OS_PROJECT_ID
unset OS_PROJECT_NAME
unset OS_PROJECT_DOMAIN_ID
unset OS_PROJECT_DOMAIN_NAME
unset OS_USER_ID
unset OS_USER_NAME
unset OS_USER_DOMAIN_ID
unset OS_USER_DOMAIN_NAME
unset OS_REGION_ID
unset OS_REGION_NAME
# fill in the project, user, and endpoint details
export PROJECT_ID=
export PROJECT_NAME=
export OS_USERNAME=
export OS_PASSWORD=
export OS_REGION_NAME=&quot;RegionOne&quot;
export OS_AUTH_URL=&quot;http://controller1:5000/v2.0&quot;
export OS_AUTH_VERSION=&quot;2.0&quot;</code></pre></li>
<li><p>Keystone v3</p>
<pre><code># unset any variables used
unset OS_PROJECT_ID
unset OS_PROJECT_NAME
unset OS_PROJECT_DOMAIN_ID
unset OS_PROJECT_DOMAIN_NAME
unset OS_USER_ID
unset OS_USER_NAME
unset OS_USER_DOMAIN_ID
unset OS_USER_DOMAIN_NAME
unset OS_REGION_ID
unset OS_REGION_NAME
# fill in the project, user, and endpoint details
export OS_PROJECT_ID=
export OS_PROJECT_NAME=
export OS_PROJECT_DOMAIN_NAME=&quot;default&quot;
export OS_USERID=
export OS_USERNAME=
export OS_PASSWORD=
export OS_USER_DOMAIN_NAME=&quot;default&quot;
export OS_REGION_NAME=&quot;RegionOne&quot;
export OS_AUTH_URL=&quot;http://controller1:5000/v3&quot;
export OS_AUTH_VERSION=&quot;3&quot;</code></pre></li>
<li><p>Source the credential file to load it into the shell environment:</p>
<pre><code>$ source &lt;USER_CREDENTIALS_FILE&gt;.sh</code></pre></li>
<li><p>View the available command line options.</p>
<pre><code>$ openstack help</code></pre>
<pre><code>$ openstack help &lt;OPTION&gt;</code></pre></li>
</ul>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;OpenStack Command Line.&quot; OpenStack Documentation. Accessed October 16, 2016. http://docs.openstack.org/developer/python-openstackclient/man/openstack.html</li>
</ol>
<h1 id="automation">Automation</h1>
<p>Automating deployments can be accomplished in a few ways. The built-in OpenStack way is via Orhcestration as a Service (OaaS), typically handled by Heat. It is also possible to use Ansible or Vagrant to automate OpenStack deploys.</p>
<h2 id="automation---heat">Automation - Heat</h2>
<p>Heat is used to orchestrate the deployment of multiple OpenStack components at once. It can also install and configure software on newly built instances.</p>
<h2 id="automation---heat---resources">Automation - Heat - Resources</h2>
<p>Heat templates are made of multiple resources. All of the different resource types are listed here <a href="http://docs.openstack.org/developer/heat/template_guide/openstack.html" class="uri">http://docs.openstack.org/developer/heat/template_guide/openstack.html</a>. Resources use properties to create a component. If no name is specified (for example, a network name), a random string will be used. Most properties also accept either an exact ID of a resource or a reference to a dynamically generated resource (which will provide it's ID once it has been created).</p>
<p>This section will go over examples of the more common modules.</p>
<p>Syntax:</p>
<pre><code>&lt;DESCRIPTIVE_OBJECT_NAME&gt;:
    type: &lt;HEAT_RESOURCE_TYPE&gt;
    properties:
        &lt;PROPERTY_1&gt;: &lt;VALUE_1&gt;
        &lt;PROPERTY_2&gt;: &lt;VALUE_2&gt;</code></pre>
<p>For referencing created resources (for example, creating a subnet in a created network) the &quot;get_resource&quot; function should be used.</p>
<pre><code>{ get_resource: &lt;OBJECT_NAME&gt; }</code></pre>
<ul>
<li><p>Create a network, assigned to the &quot;internal_network&quot; object.</p>
<pre><code>internal_network: {type: &#39;OS::Neutron::Net&#39;}</code></pre></li>
<li><p>Create a subnet for the created network. Required properties: network name or ID.</p>
<pre><code>internal_subnet:
type: OS::Neutron::Subnet
properties:
  ip_version: 4
  cidr: 10.0.0.0/24
  dns_nameservers: [8.8.4.4, 8.8.8.8]
  network_id: {get_resource: internal_network}</code></pre></li>
<li><p>Create a port. This object can be used during the instance creation. Required properties: network name or ID.</p>
<pre><code>subnet_port:
type: OS::Neutron::Port
properties:
    fixed_ips:
        - subnet_id: {get_resource: internal_subnet}
    network: {get_resource: internal_network}</code></pre></li>
<li><p>Create a router associated with the public &quot;ext-net&quot; network.</p>
<pre><code>external_router:
type: OS::Neutron::Router
properties:
    external_gateway_info: [ network: ext-net ]</code></pre></li>
<li><p>Attach a port from the network to the router.</p>
<pre><code>external_router_interface:
type: OS::Neutron::RouterInterface
properties:
    router: {get_resource: external_router}
    subnet: {get_resource: internal_subnet}</code></pre></li>
<li><p>Create a key pair called &quot;HeatKeyPair.&quot; Required property: name.</p>
<pre><code>ssh_keys:
type: OS::Nova::KeyPair
properties:
    name: HeatKeyPair
    public_key: HeatKeyPair
    save_private_key: true</code></pre></li>
<li><p>Create an instance using the &quot;m1.small&quot; flavor, &quot;centos7&quot; image, assign the subnet port creaetd by &quot;subnet_port&quot; and use the &quot;default&quot; security group.</p>
<pre><code>instance_creation:
  type: OS::Nova::Server
  properties:
flavor: m1.small
image: centos7
networks:
    - port: {get_resource: subnet_port}
security_groups: {default}</code></pre></li>
<li><p>Allocate an IP from the &quot;ext-net&quot; floating IP pool.</p>
<pre><code>floating_ip:
type: OS::Neutron::FloatingIP
properties: {floating_network: ext-net}</code></pre></li>
<li><p>Allocate a a floating IP to the created instance from a &quot;instance_creation&quot; function. Alternatively, a specific instance's ID can be defined here.</p>
<pre><code>floating_ip_association:
type: OS::Nova::FloatingIPAssociation
properties:
    floating_ip: {get_resource: floating_ip}
    server_id: {get_resource: instance_creation}</code></pre></li>
</ul>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;OpenStack Orchestration In Depth, Part I: Introduction to Heat.&quot; Accessed September 24, 2016. November 7, 2014. https://developer.rackspace.com/blog/openstack-orchestration-in-depth-part-1-introduction-to-heat/</li>
</ol>
<h2 id="automation---heat---parameters">Automation - Heat - Parameters</h2>
<p>Parameters allow users to input custom variables for Heat templates.</p>
<p>Options: * type = The input type. This can be a string, number, JSON, a comma seperated list or a boolean. * label = String. The text presented to the end-user for the fillable entry. * description = String. Detailed information about the parameter. * default = A default value for the parameter. * constraints = A parameter has to match a specified constrant. Any number of constraints can be used from the available ones below. * length = How long a string can be. * range = How big a number can be. * allowed_values = Allow only one of these specific values to be used. * allowed_pattern = Allow only a value matching a regular expression. * custom_constraint = A full list of custom service constraints can be found at <a href="#http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#custom-constraint">http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#custom-constraint</a>. * hidden = Boolean. Specify if the text entered should be hidden or not. Default: false. * immutable = Boolean. Specify whether this variable can be changed. Default: false.</p>
<p>Syntax:</p>
<pre><code>parameters:
    &lt;CUSTOM_NAME&gt;:
        type: string
        label: &lt;LABEL&gt;
        description: &lt;DESCRIPTION&gt;
        default: &lt;DEFAULT_VALUE&gt;
        constraints:
            - length: { min: &lt;MINIMUM_NUMBER&gt;, max: &lt;MAXIMUM_NUMBER&gt; }
            - range: { min: &lt;MINIMUM_NUMBER&gt;, max: &lt;MAXIMUM_NUMBER&gt; }
            - allowed_values: [ &lt;VALUE1&gt;, &lt;VALUE2&gt;, &lt;VALUE3&gt; ]
            - allowed_pattern: &lt;REGULAR_EXPRESSION&gt;
            - custom_constrant: &lt;CONSTRAINT&gt;
        hidden: &lt;BOOLEAN&gt;
        immutable: &lt;BOOLEAN&gt;</code></pre>
<p>For referencing this parameter elsewhere in the Heat template, use this syntax for the variable:</p>
<pre><code>{ get_param: &lt;CUSTOM_NAME&gt; }</code></pre>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Heat Orchestration Template (HOT) specification.&quot; OpenStack Developer Documentation. October 21, 2016. Accessed October 22, 2016. http://docs.openstack.org/developer/heat/template_guide/hot_spec.html</li>
</ol>
<h2 id="automation---vagrant">Automation - Vagrant</h2>
<p>Vagrant is a tool to automate the deployment of virtual machines. A &quot;Vagrantfile&quot; file is used to initalize the instance. An example is provided below. Note that Vagrant currently does not support Keystone's v3 API.</p>
<pre><code>require &#39;vagrant-openstack-provider&#39;

Vagrant.configure(&#39;2&#39;) do |config|

  config.vm.box       = &#39;vagrant-openstack&#39;
  config.ssh.username = &#39;cloud-user&#39;

  config.vm.provider :openstack do |os|
    os.openstack_auth_url = &#39;http://controller1/v2.0/tokens&#39;
    os.username           = &#39;openstackUser&#39;
    os.password           = &#39;openstackPassword&#39;
    os.tenant_name        = &#39;myTenant&#39;
    os.flavor             = &#39;m1.small&#39;
    os.image              = &#39;centos&#39;
    os.networks           = &quot;vagrant-net&quot;
    os.floating_ip_pool   = &#39;publicNetwork&#39;
    os.keypair_name       = &quot;private_key&quot;
  end
end</code></pre>
<p>Once those settings are configured for the end user's cloud environment, it can be created by running:</p>
<pre><code>$ vagrant up --provider=openstack</code></pre>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Vagrant OpenStack Cloud Provider.&quot; GitHub - ggiamarchi. Accessed September 24, 2016. April 30, 2016. https://github.com/ggiamarchi/vagrant-openstack-provider</li>
</ol>
<h1 id="testing">Testing</h1>
<h2 id="testing---tempest">Testing - Tempest</h2>
<p>Tempest is used to query all of the different APIs in use. This helps to validate the functionality of OpenStack. This software is a rolling release aimed towards verifying the latest OpenStack release in development but it should also work for older versions as well.</p>
<p>The sample configuration flie &quot;/etc/tempest/tempest.conf.sample&quot; should be copied to &quot;/etc/tempest/tempest.conf&quot; and then modified. If it is not available then the latest configuration file can be downloaded from one of thes sources: * http://docs.openstack.org/developer/tempest/sampleconf.html * http://docs.openstack.org/developer/tempest/_static/tempest.conf.sample</p>
<ul>
<li><p>Provide credentials to a user with the &quot;admin&quot; role.</p>
<pre><code>[auth]
admin_username
admin_password
admin_project_name
admin_domain_name
default_credentials_domain_name = Default</code></pre></li>
<li><p>Specify the Keystone version to use. Valid options are &quot;v2&quot; and &quot;v3.&quot;</p>
<pre><code>[identity]
auth_version</code></pre></li>
<li><p>Provide the admin Keystone endpoint for v2 (uri) or v3 (uri_v3).</p>
<pre><code>[identity]
uri
uri_v3</code></pre></li>
<li><p>Two different size flavor IDs should be given.</p>
<pre><code>[compute]
flavor_ref
flavor_ref_alt</code></pre></li>
<li><p>Two different image IDs should be given.</p>
<pre><code>[compute]
image_ref
image_ref_alt</code></pre></li>
<li><p>Define what services should be tested for the specific cloud.</p>
<pre><code>[service_available]
cinder = true
neutron = true
glance = true
swift = false
nova = true
heat = false
sahara = false
ironic = false</code></pre></li>
</ul>
<p>[1]</p>
<p>Source:</p>
<ol style="list-style-type: decimal">
<li>&quot;Tempest Configuration Guide.&quot; Sep 14th, 2016. http://docs.openstack.org/developer/tempest/configuration.html</li>
</ol>
<h1 id="performance">Performance</h1>
<p>A few general tips for getting the fastest OpenStack performance.</p>
<ul>
<li>KeyStone</li>
<li>Switch to Fernet keys.
<ul>
<li>Creation of tokens is significantly faster.</li>
<li>Refer to <a href="#configurations---keystone---token-provider">Configurations - Keystone - Token Provider</a>.</li>
</ul></li>
<li>Neutron</li>
<li>Use distributed virtual routing (DVR).
<ul>
<li>This offloads a lot of networking resources onto the compute nodes.</li>
</ul></li>
<li>General</li>
<li>Utilize /etc/hosts.
<ul>
<li>Ensure that all of your domain names (including the public domains) are listed in the /etc/hosts. This avoids a performance hit from DNS lookups. Alternatively, consider setting up a recursive DNS server on the controller nodes.</li>
</ul></li>
<li>Use memcache.
<ul>
<li>This is generally configured by an option called &quot;memcache_servers&quot; in the configuration files for most services. Consider using &quot;CouchBase&quot; for its ease of clustering and redudancy support.</li>
</ul></li>
</ul>
