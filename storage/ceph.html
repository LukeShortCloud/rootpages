

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Ceph &mdash; Root Pages  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="File Systems" href="file_systems.html" />
    <link rel="prev" title="Bootloaders" href="bootloaders.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Root Pages
          

          
          </a>

          
            
            
              <div class="version">
                2021.04.01
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Administration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../administration/authentication.html">Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../administration/chromebook.html">Chromebook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../administration/graphics.html">Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../administration/linux.html">Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../administration/macs.html">Macs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../administration/mail_servers.html">Mail Servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../administration/operating_systems.html">Operating Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../administration/package_managers.html">Package Managers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../administration/security.html">Security</a></li>
</ul>
<p class="caption"><span class="caption-text">Automation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../automation/ansible.html">Ansible</a></li>
<li class="toctree-l1"><a class="reference internal" href="../automation/puppet.html">Puppet</a></li>
</ul>
<p class="caption"><span class="caption-text">Computer Hardware</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../computer_hardware/graphics_cards.html">Graphics Cards</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computer_hardware/laptops.html">Laptops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computer_hardware/monitors.html">Monitors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computer_hardware/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computer_hardware/storage_devices.html">Storage Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computer_hardware/webcams.html">Webcams</a></li>
</ul>
<p class="caption"><span class="caption-text">HTTP</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../http/clustering.html">Clustering and High Availability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../http/cms.html">Content Management Systems (CMSs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../http/databases.html">Databases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../http/http_servers.html">HTTP Servers</a></li>
</ul>
<p class="caption"><span class="caption-text">Networking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../networking/dns_servers.html">DNS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../networking/networking_hardware.html">Networking (Hardware)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../networking/linux.html">Linux Networking</a></li>
</ul>
<p class="caption"><span class="caption-text">Observation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../observation/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../observation/monitoring.html">Monitoring</a></li>
</ul>
<p class="caption"><span class="caption-text">OpenStack</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../openstack/openstack.html">OpenStack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../openstack/developer.html">OpenStack Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../openstack/kolla.html">Kolla</a></li>
<li class="toctree-l1"><a class="reference internal" href="../openstack/openstack-ansible.html">OpenStack-Ansible</a></li>
<li class="toctree-l1"><a class="reference internal" href="../openstack/tripleo.html">TripleO</a></li>
</ul>
<p class="caption"><span class="caption-text">Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../programming/c_and_c%2B%2B.html">C and C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming/devops.html">DevOps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming/go.html">Go</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming/packaging.html">Packaging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming/python.html">Python 3</a></li>
</ul>
<p class="caption"><span class="caption-text">Storage</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="backup_and_recovery.html">Backup and Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="bootloaders.html">Bootloaders</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Ceph</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#releases">Releases</a></li>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quick">Quick</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ceph-ansible-octopus">ceph-ansible (&lt;= Octopus)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#crush-map">CRUSH Map</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#devices">Devices</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bucket-types">Bucket Types</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bucket-instances">Bucket Instances</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rules">Rules</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#repair">Repair</a></li>
<li class="toctree-l2"><a class="reference internal" href="#libvirt">libvirt</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cephfs">CephFS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#history">History</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bibliography">Bibliography</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="file_systems.html">File Systems</a></li>
</ul>
<p class="caption"><span class="caption-text">Virtualization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../virtualization/containers.html">Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../virtualization/kubernetes_administration.html">Kubernetes Administration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../virtualization/kubernetes_development.html">Kubernetes Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../virtualization/virtual_machines.html">Virtual Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../virtualization/wine.html">Wine</a></li>
</ul>
<p class="caption"><span class="caption-text">Commands</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../commands/clustering.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/compression.html">Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/configuration_management.html">Configuration Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/firewalls.html">Firewalls</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/hardware.html">Hardware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/openstack.html">OpenStack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/package_managers.html">Package Managers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/permissions.html">Permissions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/phones.html">Phones</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/security.html">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/software_code_management.html">Source Code Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/storage.html">Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/text_editors.html">Text Editors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../commands/virtualization.html">Virtualization</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Root Pages</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">&lt;no title&gt;</a> &raquo;</li>
        
      <li>Ceph</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/storage/ceph.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="ceph">
<h1><a class="toc-backref" href="#id1">Ceph</a><a class="headerlink" href="#ceph" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#ceph" id="id1">Ceph</a></p>
<ul>
<li><p><a class="reference internal" href="#introduction" id="id2">Introduction</a></p></li>
<li><p><a class="reference internal" href="#releases" id="id3">Releases</a></p></li>
<li><p><a class="reference internal" href="#installation" id="id4">Installation</a></p>
<ul>
<li><p><a class="reference internal" href="#quick" id="id5">Quick</a></p></li>
<li><p><a class="reference internal" href="#ceph-ansible-octopus" id="id6">ceph-ansible (&lt;= Octopus)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#crush-map" id="id7">CRUSH Map</a></p>
<ul>
<li><p><a class="reference internal" href="#devices" id="id8">Devices</a></p></li>
<li><p><a class="reference internal" href="#bucket-types" id="id9">Bucket Types</a></p></li>
<li><p><a class="reference internal" href="#bucket-instances" id="id10">Bucket Instances</a></p></li>
<li><p><a class="reference internal" href="#rules" id="id11">Rules</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#repair" id="id12">Repair</a></p></li>
<li><p><a class="reference internal" href="#libvirt" id="id13">libvirt</a></p></li>
<li><p><a class="reference internal" href="#cephfs" id="id14">CephFS</a></p></li>
<li><p><a class="reference internal" href="#history" id="id15">History</a></p></li>
<li><p><a class="reference internal" href="#bibliography" id="id16">Bibliography</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction">
<h2><a class="toc-backref" href="#id2">Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Ceph is a storage project that is sponsored by The Linux Foundation. It has developed a storage system that uses Reliable Autonomic Distributed Object Store (RADOS) to provide scalable, fast, and reliable software-defined storage by storing files as objects and calculating their location on the fly. Failovers will even happen automatically so no data is lost. By default, there are 3 replicas of each file stored on an OSD.</p>
<p>Vocabulary:</p>
<ul class="simple">
<li><p>Object Storage Device (OSD) = The device that stores data.</p></li>
<li><p>OSD Daemon = Handles storing all user data as objects.</p></li>
<li><p>Ceph Block Device (RBD) = Provides a block device over the network,
similar in concept to iSCSI.</p></li>
<li><p>Ceph Object Gateway = A RESTful API which works with Amazon S3 and
OpenStack Swift.</p></li>
<li><p>Ceph Monitors (MONs) = Store and provide a map of data locations.</p></li>
<li><p>Ceph Metadata Server (MDS) = Provides metadata about file system
hierarchy for CephFS. This is not required for RBD or RGW.</p></li>
<li><p>Ceph File System (CephFS) = A POSIX-compliant distributed file system
with unlimited size.</p></li>
<li><p>Controlled Replication Under Scalable Hash (CRUSH) = Uses an
algorithm to provide metadata about an object’s location.</p></li>
<li><p>Placement Groups (PGs) = Object storage data.</p></li>
</ul>
<p>Ceph monitor nodes have a master copy of a cluster map. This contains 5
separate maps that have information about data location and the
cluster’s status. If an OSD fails, the monitor daemon will automatically
reorganize everything and provided end-user’s with an updated cluster
map.</p>
<p>Cluster map:</p>
<ul class="simple">
<li><p>Monitor map = The cluster fsid (uuid), position, name, address and
port of each monitor server.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">sudo</span> <span class="pre">ceph</span> <span class="pre">mon</span> <span class="pre">dump</span></code></p></li>
</ul>
</li>
<li><p>OSD map = The cluster fsid, available pools, PG numbers, and OSDs
current status.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">sudo</span> <span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">dump</span></code></p></li>
</ul>
</li>
<li><p>PG map = PG version, PG ID, ratios, and data usage statistics.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">sudo</span> <span class="pre">ceph</span> <span class="pre">pg</span> <span class="pre">dump</span></code></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#network---ceph---crush-map">CRUSH map</a> = Storage devices,
physical locations, and rules for storing objects. It is recommended
to tweak this for production clusters.</p></li>
<li><p>MDS map</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">sudo</span> <span class="pre">ceph</span> <span class="pre">fs</span> <span class="pre">dump</span></code></p></li>
</ul>
</li>
</ul>
<p>When the end-user asks for a file, that name is combined with it’s PG ID
and then CRUSH hashes it to find the exact location of it on all of the
OSDs. The master OSD for that file serves the content. [1]</p>
<p>For OSD nodes, it is recommend that the operating system is on two disks in a RAID 1. All of the over disks can be used for OSD or journal/metadata services.</p>
<p>As of Luminous release, the new <code class="docutils literal notranslate"><span class="pre">mgr</span></code> (managers) monitoring service is required. It helps to collect metrics about the cluster. It should be running on all of the monitor nodes. <a class="reference external" href="https://docs.ceph.com/docs/luminous/release-notes/">https://docs.ceph.com/docs/luminous/release-notes/</a></p>
<p>The current back-end for handling data storage is FileStore. When data
is written to a Ceph OSD, it is first fully written to the OSD journal.
This is a separate partition that can be on the same drive or a
different drive. It is faster to have the journal on an SSD if the OSD
drive is a regular spinning-disk drive.</p>
<p>The new BlueStore back-end was released as a technology preview in the Ceph Jewel release. In the Luminous release, it had became the default data storage handler. This helps to overcome the double write penalty of FileStore by writing the the data to the block device first and then updating the metadata of the data’s location. That means that in some cases, BlueStore is twice as fast as FileStore. All of the metadata is also stored in the fast RocksDB key-value store. File systems are no longer required for OSDs because BlueStore writes data directly to the block device of the hard drive. [2] It is recommended to have a 3:1 ratio for OSDS to BlueStore journals/metadata. The metadata drives should be a fast storage medium such as an SSD or NVMe.</p>
<p><code class="docutils literal notranslate"><span class="pre">ceph-volume</span></code> is a tool for automagically figuring out which disks to use for journals/metadata or OSDs. It replaces ceph-disk and supports BlueStore. It does not support loopback devices. The logic it normally follows is:</p>
<ul class="simple">
<li><p>1 OSD per HDD</p></li>
<li><p>2 OSDs per SSD</p></li>
<li><p>HDD + SSD = HDD OSDs and SSD metadata</p></li>
</ul>
<p>The optimal number of PGs is found be using this equation (replacing the number of OSD daemons and how many replicas are set). This number should be rounded up to the next power of 2. <a class="reference external" href="https://ceph.io/pgcalc/">PGCalc</a> is an online utility/calculator to help automatically determine this value.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Total</span> <span class="n">PGs</span> <span class="o">=</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">NUMBER_OF_OSDS</span><span class="o">&gt;</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span> <span class="o">/</span> <span class="o">&lt;</span><span class="n">REPLICA_COUNT</span><span class="o">&gt;</span> <span class="o">/</span> <span class="o">&lt;</span><span class="n">NUMBER_OF_POOLS</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OSD</span> <span class="n">count</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span> <span class="n">replica</span> <span class="n">count</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">pool</span> <span class="n">count</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">Run</span> <span class="n">the</span> <span class="n">calculations</span><span class="p">:</span> <span class="mi">1000</span> <span class="o">=</span> <span class="p">(</span><span class="mi">30</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">1</span>
<span class="n">Find</span> <span class="n">the</span> <span class="nb">next</span> <span class="n">highest</span> <span class="n">power</span> <span class="n">of</span> <span class="mi">2</span><span class="p">:</span> <span class="mi">2</span><span class="o">^</span><span class="mi">10</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="mi">1000</span> <span class="o">=&lt;</span> <span class="mi">1024</span>
<span class="n">Total</span> <span class="n">PGs</span> <span class="o">=</span> <span class="mi">1024</span>
</pre></div>
</div>
<p>With Ceph’s configuration, the Placement Group for Placement purpose
(PGP) should be set to the same PG number. PGs are the number of number
of times a file should be split. This change only makes the Ceph cluster
rebalance when the PGP count is increased.</p>
<ul class="simple">
<li><p>New pools:</p></li>
</ul>
<p>File:  /etc/ceph/ceph.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[global]</span>
<span class="na">osd pool default pg num</span> <span class="o">=</span> <span class="s">&lt;OPTIMAL_PG_NUMBER&gt;</span>
<span class="na">osd pool default pgp num</span> <span class="o">=</span> <span class="s">&lt;OPTIMAL_PG_NUMBER&gt;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Existing pools:</p></li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph osd pool <span class="nb">set</span> &lt;POOL&gt; pg_num &lt;OPTIMAL_PG_NUMBER&gt;
$ sudo ceph osd pool <span class="nb">set</span> &lt;POOL&gt; pgp_num &lt;OPTIMAL_PG_NUMBER&gt;
</pre></div>
</div>
<p>Cache pools can be configured used to cache files onto faster drives.
When a file is continually being read, it will be copied to the faster
drive. When a file is first written, it will go to the faster drives.
After a period of time of lesser use, those files will be moved to the
slow drives. [3]</p>
<p>For testing, the “cephx” authentication protocols can temporarily be
disabled. This will require a restart of all of the Ceph services.
Re-enable <code class="docutils literal notranslate"><span class="pre">cephx</span></code> by setting these values from “none” to “cephx.” [4]</p>
<p>File: /etc/ceph/ceph.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[global]</span>
<span class="na">auth cluster required</span> <span class="o">=</span> <span class="s">none</span>
<span class="na">auth service required</span> <span class="o">=</span> <span class="s">none</span>
<span class="na">auth client required</span> <span class="o">=</span> <span class="s">none</span>
</pre></div>
</div>
</div>
<div class="section" id="releases">
<h2><a class="toc-backref" href="#id3">Releases</a><a class="headerlink" href="#releases" title="Permalink to this headline">¶</a></h2>
<p>Starting with the Luminous 12 release, all versions are supported for two years. A new release comes out every year. [13]</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;RELEASE_NAME&gt;</span> <span class="pre">&lt;RELEASE_NUMBER&gt;</span> <span class="pre">=</span> <span class="pre">&lt;RELEASE_DATE&gt;</span></code></p></li>
<li><p>Luminous 12 = 2017-02</p></li>
<li><p>Mimic 13 = 2018-05</p></li>
<li><p>Nautilus 14 = 2019-03</p></li>
<li><p>Octopus 15 = 2020-03</p></li>
</ul>
</div>
<div class="section" id="installation">
<h2><a class="toc-backref" href="#id4">Installation</a><a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Ceph Requirements:</p>
<ul class="simple">
<li><p>Fast CPU for OSD and metadata nodes.</p></li>
<li><p>1GB RAM per 1TB of Ceph OSD storage, per OSD daemon.</p></li>
<li><p>1GB RAM per monitor daemon.</p></li>
<li><p>1GB RAM per metadata daemon.</p></li>
<li><p>An odd number of monitor nodes (starting at least 3 for high
availability and quorum). [5]</p></li>
</ul>
<div class="section" id="quick">
<h3><a class="toc-backref" href="#id5">Quick</a><a class="headerlink" href="#quick" title="Permalink to this headline">¶</a></h3>
<p>This example demonstrates how to deploy a 3 node Ceph cluster with both
the monitor and OSD services. In production, monitor servers should be
separated from the OSD storage nodes.</p>
<ul>
<li><p>Create a new Ceph cluster group, by default called “ceph.”</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph-deploy new &lt;SERVER1&gt;
</pre></div>
</div>
</li>
<li><p>Install the latest LTS release for production environments on the
specified servers. SSH access is required.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph-deploy install --release jewel &lt;SERVER1&gt; &lt;SERVER2&gt; &lt;SERVER3&gt;
</pre></div>
</div>
</li>
<li><p>Initialize the first monitor.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph-deploy mon create-initial &lt;SERVER1&gt;
</pre></div>
</div>
</li>
<li><p>Install the monitor service on the other nodes.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph-deploy mon create &lt;SERVER2&gt; &lt;SERVER3&gt;
</pre></div>
</div>
</li>
<li><p>List the available hard drives from all of the servers. It is
recommended to have a fully dedicated drive, not a partition, for
each Ceph OSD.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph-deploy disk list &lt;SERVER1&gt; &lt;SERVER2&gt; &lt;SERVER3&gt;
</pre></div>
</div>
</li>
<li><p>Carefully select the drives to use. Then use the “disk zap” arguments
to zero out the drive before use.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph-deploy disk zap &lt;SERVER1&gt;:&lt;DRIVE&gt; &lt;SERVER2&gt;:&lt;DRIVE&gt; &lt;SERVER3&gt;:&lt;DRIVE&gt;
</pre></div>
</div>
</li>
<li><p>Prepare and deploy the OSD service for the specified drives. The
default file system is XFS, but Btrfs is much feature-rich with
technologies such as copy-on-write (CoW) support.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph-deploy osd create --fs-type btrfs &lt;SERVER1&gt;:&lt;DRIVE&gt; &lt;SERVER2&gt;:&lt;DRIVE&gt; &lt;SERVER3&gt;:&lt;DRIVE&gt;
</pre></div>
</div>
</li>
<li><p>Verify it’s working.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph status
</pre></div>
</div>
</li>
</ul>
<p>[6]</p>
</div>
<div class="section" id="ceph-ansible-octopus">
<h3><a class="toc-backref" href="#id6">ceph-ansible (&lt;= Octopus)</a><a class="headerlink" href="#ceph-ansible-octopus" title="Permalink to this headline">¶</a></h3>
<p>The ceph-ansible project is used to deploy and update Ceph clusters using Ansible. It is deprecated and replaced by <a class="reference external" href="https://docs.ceph.com/docs/master/cephadm/">cephadm</a>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo git clone https://github.com/ceph/ceph-ansible/
$ sudo <span class="nb">cd</span> ceph-ansible/
</pre></div>
</div>
<p>Configure the Ansible inventory hosts file. This should contain the SSH
connection details to access the relevant servers.</p>
<p>Inventory hosts:</p>
<ul class="simple">
<li><p>[mons] = Monitors for tracking and locating object storage data.</p></li>
<li><p>[osds] = Object storage device nodes for storing the user data.</p></li>
<li><p>[mdss] = Metadata servers for CephFS. (Optional)</p></li>
<li><p>[rwgs] = RADOS Gateways for Amazon S3 or OpenStack Swift object
storage API support. (Optional)</p></li>
</ul>
<p>Example inventory:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">ceph_monitor_01 ansible_host</span><span class="o">=</span><span class="s">192.168.20.11</span>
<span class="na">ceph_monitor_02 ansible_host</span><span class="o">=</span><span class="s">192.168.20.12</span>
<span class="na">ceph_monitor_03 ansible_host</span><span class="o">=</span><span class="s">192.168.20.13</span>
<span class="na">ceph_osd_01 ansible_host</span><span class="o">=</span><span class="s">192.168.20.101 ansible_port=2222</span>
<span class="na">ceph_osd_02 ansible_host</span><span class="o">=</span><span class="s">192.168.20.102 ansible_port=2222</span>
<span class="na">ceph_osd_03 ansible_host</span><span class="o">=</span><span class="s">192.168.20.103 ansible_port=2222</span>

<span class="k">[mons]</span>
<span class="na">ceph_monitor_01</span>
<span class="na">ceph_monitor_02</span>
<span class="na">ceph_monitor_03</span>

<span class="k">[osds]</span>
<span class="na">ceph_osd_01</span>
<span class="na">ceph_osd_02</span>
<span class="na">ceph_osd_03</span>
</pre></div>
</div>
<p>Copy the sample configurations and modify the variables.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo cp site.yml.sample site.yml
$ sudo <span class="nb">cd</span> group_vars/
$ sudo cp all.yml.sample all.yml
$ sudo cp mons.yml.sample mons.yml
$ sudo cp osds.yml.sample osds.yml
</pre></div>
</div>
<p>Common variables:</p>
<ul class="simple">
<li><p>group_vars/all.yml = Global variables.</p>
<ul>
<li><p>ceph_origin = Specify how to install the Ceph software.</p>
<ul>
<li><p>upstream = Use the official repositories.</p></li>
<li><p>Upstream related variables:</p>
<ul>
<li><p>ceph_dev: Boolean value. Use a development branch of Ceph
from GitHub.</p></li>
<li><p>ceph_dev_branch = The exact branch or commit of Ceph from
GitHub to use.</p></li>
<li><p>ceph_stable = Boolean value. Use a stable release of Ceph.</p></li>
<li><p>ceph_stable_release = The release name to use. The LTS
“jewel” release is recommended.</p></li>
</ul>
</li>
<li><p>distro = Use repositories already present on the system.
ceph-ansible will not install Ceph repositories with this
method, they must already be installed.</p></li>
</ul>
</li>
<li><p>ceph_release_num = If “ceph_stable” is not defined, use any
specific major release number.</p>
<ul>
<li><p>9 = infernalis</p></li>
<li><p>10 = jewel</p></li>
<li><p>11 = kraken</p></li>
</ul>
</li>
</ul>
</li>
<li><p>group_vars/osds.yml = Object storage daemon variables.</p>
<ul>
<li><p>devices = A list of drives to use for each OSD daemon.</p></li>
<li><p>osd_auto_discovery = Boolean value. Default: false. Instead of
manually specifying devices to use, automatically use any drive
that does not have a partition table.</p></li>
<li><p>OSD option #1:</p>
<ul>
<li><p>journal_collocation = Boolean value. Default: false. Use the
same drive for journal and data storage.</p></li>
</ul>
</li>
<li><p>OSD option #2:</p>
<ul>
<li><p>raw_multi_journal = Boolean value. Default: false. Store
journals on different hard drives.</p></li>
<li><p>raw_journal_devices = A list of devices to use for
journaling.</p></li>
</ul>
</li>
<li><p>OSD option #3:</p>
<ul>
<li><p>osd_directory = Boolean value. Default: false. Use a specified
directory for OSDs. This assumes that the end-user has already
partitioned the drive and mounted it to
<code class="docutils literal notranslate"><span class="pre">/var/lib/ceph/osd/&lt;OSD_NAME&gt;</span></code> or a custom directory.</p></li>
<li><p>osd_directories = The directories to use for OSD storage.</p></li>
</ul>
</li>
<li><p>OSD option #4:</p>
<ul>
<li><p>bluestore: Boolean value. Default: false. Use the new and
experimental BlueStore file store that can provide twice the
performance for drives that have both a journal and OSD for
Ceph.</p></li>
</ul>
</li>
<li><p>OSD option #5:</p>
<ul>
<li><p>dmcrypt_journal_collocation = Use Linux’s “dm-crypt” to
encrypt objects when both the journal and data are stored on
the same drive.</p></li>
</ul>
</li>
<li><p>OSD option #6:</p>
<ul>
<li><p>dmcrypt_dedicated_journal = Use Linux’s “dm-crypt” to encrypt
objects when both the journal and data are stored on the
different drives.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Finally, run the Playbook to deploy the Ceph cluster.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ansible-playbook -i production site.yml
</pre></div>
</div>
<p>[7]</p>
</div>
</div>
<div class="section" id="crush-map">
<h2><a class="toc-backref" href="#id7">CRUSH Map</a><a class="headerlink" href="#crush-map" title="Permalink to this headline">¶</a></h2>
<p>CRUSH maps are used to keep track of OSDs, physical locations of
servers, and it defines how to replicate objects.</p>
<p>These maps are divided into four main parts:</p>
<ul class="simple">
<li><p>Devices = The list of each OSD daemon in the cluster.</p></li>
<li><p>Bucket Types = Definitions that can group OSDs into groups with their
own location and weights based on servers, rows, racks, datacenters,
etc.</p></li>
<li><p>Bucket Instances = A bucket instance is created by specifying a
bucket type and one or more OSDs.</p></li>
<li><p>Rules = Rules can be defined to configure which bucket instances will
be used for reading, writing, and/or replicating data.</p></li>
</ul>
<p>A binary of the configuration must be saved and then decompiled before
changes can be made. Then the file must be recompiled for the updates to
be loaded.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph osd getcrushmap -o &lt;NEW_COMPILED_FILE&gt;
$ sudo crushtool -d &lt;NEW_COMPILED_FILE&gt; -o &lt;NEW_DECOMPILED_FILE&gt;
$ sudo vim &lt;NEW_DECOMPILED_FILE&gt;<span class="sb">`</span>
$ sudo crushtool -c &lt;NEW_DECOMPILED_FILE&gt; -o &lt;UPDATED_COMPILED_FILE&gt;
$ sudo ceph osd setcrushmap -i &lt;UPDATED_COMPILED_FILE&gt;
</pre></div>
</div>
<div class="section" id="devices">
<h3><a class="toc-backref" href="#id8">Devices</a><a class="headerlink" href="#devices" title="Permalink to this headline">¶</a></h3>
<p>Devices must follow the format of <code class="docutils literal notranslate"><span class="pre">device</span> <span class="pre">&lt;COUNT&gt;</span> <span class="pre">&lt;OSD_NAME&gt;</span></code>. These
are automatically generated but can be adjusted and new nodes can be
manually added here.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># devices</span>
<span class="n">device</span> <span class="mi">0</span> <span class="n">osd</span><span class="o">.</span><span class="mi">0</span>
<span class="n">device</span> <span class="mi">1</span> <span class="n">osd</span><span class="o">.</span><span class="mi">1</span>
<span class="n">device</span> <span class="mi">2</span> <span class="n">osd</span><span class="o">.</span><span class="mi">2</span>
</pre></div>
</div>
</div>
<div class="section" id="bucket-types">
<h3><a class="toc-backref" href="#id9">Bucket Types</a><a class="headerlink" href="#bucket-types" title="Permalink to this headline">¶</a></h3>
<p>Bucket types follow a similar format of <code class="docutils literal notranslate"><span class="pre">type</span> <span class="pre">&lt;COUNT&gt;</span> <span class="pre">&lt;TYPE_NAME&gt;</span></code>.
The name of the type can be anything. The higher numbered type always
inherits the lower numbers. The default types include:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># types</span>
<span class="nb">type</span> <span class="mi">0</span> <span class="n">osd</span>
<span class="nb">type</span> <span class="mi">1</span> <span class="n">host</span>
<span class="nb">type</span> <span class="mi">2</span> <span class="n">chassis</span>
<span class="nb">type</span> <span class="mi">3</span> <span class="n">rack</span>
<span class="nb">type</span> <span class="mi">4</span> <span class="n">row</span>
<span class="nb">type</span> <span class="mi">5</span> <span class="n">pdu</span>
<span class="nb">type</span> <span class="mi">6</span> <span class="n">pod</span>
<span class="nb">type</span> <span class="mi">7</span> <span class="n">room</span>
<span class="nb">type</span> <span class="mi">8</span> <span class="n">datacenter</span>
<span class="nb">type</span> <span class="mi">9</span> <span class="n">region</span>
<span class="nb">type</span> <span class="mi">10</span> <span class="n">root</span>
</pre></div>
</div>
</div>
<div class="section" id="bucket-instances">
<h3><a class="toc-backref" href="#id10">Bucket Instances</a><a class="headerlink" href="#bucket-instances" title="Permalink to this headline">¶</a></h3>
<p>Bucket instances are used to group OSD configurations together.
Typically these should define physical locations of the OSDs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">CUSTOM_BUCKET_TYPE</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">UNIQUE_BUCKET_NAME</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="nb">id</span> <span class="o">&lt;</span><span class="n">UNIQUE_NEGATIVE_NUMBER</span><span class="o">&gt;</span>
    <span class="n">weight</span> <span class="o">&lt;</span><span class="n">FLOATING_NUMBER</span><span class="o">&gt;</span>
    <span class="n">alg</span> <span class="o">&lt;</span><span class="n">BUCKET_TYPE</span><span class="o">&gt;</span>
    <span class="nb">hash</span> <span class="mi">0</span>
    <span class="n">item</span> <span class="o">&lt;</span><span class="n">OSD_NAME</span><span class="o">&gt;</span> <span class="n">weight</span> <span class="o">&lt;</span><span class="n">FLOATING_NUMBER</span><span class="o">&gt;</span>
<span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;CUSTOM_BUCKET_TYPE&gt;</span></code> = Required. This should be one of the
user-defined bucket types.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;UNIQUE_BUCKET_NAME&gt;</span></code> = Required. A unique name that describes the
bucket.</p></li>
<li><p>id = Required. A unique negative number to identify the bucket.</p></li>
<li><p>weight = Optional. A floating/decimal number for all of the weight of
all of the OSDs in this bucket.</p></li>
<li><p>alg = Required. Choose which Ceph bucket type/method that is used to
read and write objects. This should not be confused with the
user-defined bucket types.</p>
<ul>
<li><p>Uniform = Assumes that all hardware in the bucket instance is
exactly the same so all OSDs receive the same weight.</p></li>
<li><p>List = Lists use the RUSH algorithm to read and write objects in
sequential order from the first OSD to the last. This is best
suited for data that does not need to be deleted (to avoid
rebalancing).</p></li>
<li><p>Tree = The binary search tree uses the RUSH algorithm to
efficiently handle larger amounts of data.</p></li>
<li><p>Straw = A combination of both “list” and “tree.” One of the two
bucket types will randomly be selected for operations. Replication
is fast but rebalancing will be slow.</p></li>
</ul>
</li>
<li><p>hash = Required. The hashing algorithm used by CRUSH to lookup and
store files. As of the Jewel release, only option “0” for “rjenkins1”
is supported.</p></li>
<li><p>item = Optional. The OSD name and weight for individual OSDs. This is
useful if a bucket instance has hard drives of different speeds.</p></li>
</ul>
</div>
<div class="section" id="rules">
<h3><a class="toc-backref" href="#id11">Rules</a><a class="headerlink" href="#rules" title="Permalink to this headline">¶</a></h3>
<p>By modifying the CRUSH map, replication can be configured to go to a
different drive, server, chassis, row, rack, datacenter, etc.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rule</span> <span class="o">&lt;</span><span class="n">RULE_NAME</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="n">ruleset</span> <span class="o">&lt;</span><span class="n">RULESET</span><span class="o">&gt;</span>
    <span class="nb">type</span> <span class="o">&lt;</span><span class="n">RULE_TYPE</span><span class="o">&gt;</span>
    <span class="n">min_size</span> <span class="o">&lt;</span><span class="n">MINIMUM_SIZE</span><span class="o">&gt;</span>
    <span class="n">max_size</span> <span class="o">&lt;</span><span class="n">MAXIMUM_SIZE</span><span class="o">&gt;</span>
    <span class="n">step</span> <span class="n">take</span> <span class="o">&lt;</span><span class="n">BUCKET_INSTANCE_NAME</span><span class="o">&gt;</span>
    <span class="n">step</span> <span class="o">&lt;</span><span class="n">CHOOSE_OPTION</span><span class="o">&gt;</span>
    <span class="n">step</span> <span class="n">emit</span>
<span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;RULE_NAME&gt;</span></code></p></li>
<li><p>ruleset = Required. An integer that can be used to reference this
ruleset by a pool.</p></li>
<li><p>type = Required. Default is “replicated.” How to handle data
replication.</p>
<ul>
<li><p>replicated = Data is replicated to different hard drives.</p></li>
<li><p>erasure = This a similar concept to RAID 5. Data is only
replicated to one drive. This option helps to save space.</p></li>
</ul>
</li>
<li><p>min_size</p></li>
<li><p>max_size</p></li>
<li><p>step take</p></li>
<li><p>step emit = Required. This signifies the end of the rule block.</p></li>
</ul>
<p>[8]</p>
</div>
</div>
<div class="section" id="repair">
<h2><a class="toc-backref" href="#id12">Repair</a><a class="headerlink" href="#repair" title="Permalink to this headline">¶</a></h2>
<p>Ceph automatically runs through a data integrity check called
“scrubbing.” This checks the health of each placement group (object).
Sometimes these can fail due to inconsistencies, commonly a mismatch in
time on the OSD servers.</p>
<p>In this example, the placement group “1.28” failed to be scrubbed. This
object exists on the 8, 11, and 20 OSD drives.</p>
<ul>
<li><p>Check the health information.</p>
<ul>
<li><p>Example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph health detail
HEALTH_ERR <span class="m">1</span> pgs inconsistent<span class="p">;</span> <span class="m">1</span> scrub errors
pg <span class="m">1</span>.28 is active+clean+inconsistent, acting <span class="o">[</span><span class="m">8</span>,11,20<span class="o">]</span>
<span class="m">1</span> scrub errors
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Manually run a repair.</p>
<ul>
<li><p>Syntax:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph pg repair &lt;PLACEMENT_GROUP&gt;
</pre></div>
</div>
</li>
<li><p>Example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph pg repair <span class="m">1</span>.28
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Find the error:</p>
<ul>
<li><p>Syntax:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo grep ERR /var/log/ceph/ceph-osd.&lt;OSD_NUMBER&gt;.log
</pre></div>
</div>
</li>
<li><p>Example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo grep ERR /var/log/ceph/ceph-osd.11.log
<span class="m">2017</span>-01-12 <span class="m">22</span>:27:52.626252 7f5b511e8700 -1 log_channel<span class="o">(</span>cluster<span class="o">)</span> log <span class="o">[</span>ERR<span class="o">]</span> : <span class="m">1</span>.27 shard <span class="m">12</span>: soid <span class="m">1</span>:e4c200f7:::rbd_data.a1e002238e1f29.000000000000136d:head candidate had a <span class="nb">read</span> error
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Find the bad file.</p>
<ul>
<li><p>Syntax:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo find /var/lib/ceph/osd/ceph-&lt;OSD_NUMBER&gt;/current/&lt;PLACEMENT_GROUP&gt;_head/ -name <span class="s1">&#39;*&lt;OBJECT_ID&gt;*&#39;</span> -ls
</pre></div>
</div>
</li>
<li><p>Example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo find /var/lib/ceph/osd/ceph-11/current/1.28_head/ -name <span class="s2">&quot;*a1e002238e1f29.000000000000136d*&quot;</span>
/var/lib/ceph/osd/ceph-11/current/1.28_head/DIR_7/DIR_2/DIR_3/rbd<span class="se">\u</span>data.b3e012238e1f29.000000000000136d__head_EF004327__1
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Stop the OSD.</p>
<ul>
<li><p>Syntax:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo systemctl stop ceph-osd@&lt;OSD_NUMBER&gt;.service
</pre></div>
</div>
</li>
<li><p>Example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo systemctl stop ceph-osd@11.service
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Flush the journal to save the current files cached in memory.</p>
<ul>
<li><p>Syntax:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph-osd -i &lt;OSD_NUMBER&gt; --flush-journal
</pre></div>
</div>
</li>
<li><p>Example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph-osd -i <span class="m">11</span> --flush-journal
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Move the bad object out of it’s current directory in the OSD.</p>
<ul>
<li><p>Example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo mv /var/lib/ceph/osd/ceph-11/current/1.28_head/DIR_7/DIR_2/DIR_3/rbd<span class="se">\\</span>udata.b3e012238e1f29.000000000000136d__head_EF004327__1 /root/ceph_osd_backups/
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Restart the OSD.</p>
<ul>
<li><p>Syntax:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo systemctl restart ceph-osd@&lt;OSD_NUMBER&gt;.service
</pre></div>
</div>
</li>
<li><p>Example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo systemctl restart ceph-osd@11.service
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Run another placement group repair.</p>
<ul>
<li><p>Syntax:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph pg repair &lt;PLACEMENT_GROUP&gt;
</pre></div>
</div>
</li>
<li><p>Example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph pg repair <span class="m">1</span>.28
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
<p>[9]</p>
</div>
<div class="section" id="libvirt">
<h2><a class="toc-backref" href="#id13">libvirt</a><a class="headerlink" href="#libvirt" title="Permalink to this headline">¶</a></h2>
<p>Virtual machines that are run via the libvirt front-end can utilize
Ceph’s RADOS block devices (RBDs) as their main disk.</p>
<ul>
<li><p>Add the network disk to the available devices in the Virsh
configuration.</p>
<div class="highlight-xml notranslate"><div class="highlight"><pre><span></span><span class="nt">&lt;devices&gt;</span>
<span class="nt">&lt;disk</span> <span class="na">type=</span><span class="s">&#39;network&#39;</span> <span class="na">device=</span><span class="s">&#39;disk&#39;</span><span class="nt">&gt;</span>
    <span class="nt">&lt;source</span> <span class="na">protocol=</span><span class="s">&#39;rbd&#39;</span> <span class="na">name=</span><span class="s">&#39;&lt;POOL&gt;/&lt;IMAGE&gt;&#39;</span><span class="nt">&gt;</span>
        <span class="nt">&lt;host</span> <span class="na">name=</span><span class="s">&#39;&lt;MONITOR_IP&gt;&#39;</span> <span class="na">port=</span><span class="s">&#39;6789&#39;</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;/source&gt;</span>
    <span class="nt">&lt;target</span> <span class="na">dev=</span><span class="s">&#39;vda&#39;</span> <span class="na">bus=</span><span class="s">&#39;virtio&#39;</span><span class="nt">/&gt;</span>
<span class="nt">&lt;/disk&gt;</span>
...
<span class="nt">&lt;/devices&gt;</span>
</pre></div>
</div>
</li>
<li><p>Authentication is required so the Ceph client credentials must be
encrypted by libvirt. This encrypted hash is called a “secret.”</p></li>
<li><p>Create a Virsh template that has a secret of type “ceph” with a
description for the end user. Optionally specify a UUID for this
secret to be associated with or else one will be generated. Example file: ceph-secret.xml</p>
<div class="highlight-xml notranslate"><div class="highlight"><pre><span></span><span class="nt">&lt;secret</span> <span class="na">ephemeral=</span><span class="s">&#39;no&#39;</span> <span class="na">private=</span><span class="s">&#39;no&#39;</span><span class="nt">&gt;</span>
<span class="nt">&lt;uuid&gt;</span>51757078-7d63-476f-8524-5d46119cfc8a<span class="nt">&lt;/uuid&gt;</span>
<span class="nt">&lt;usage</span> <span class="na">type=</span><span class="s">&#39;ceph&#39;</span><span class="nt">&gt;</span>
    <span class="nt">&lt;name&gt;</span>The Ceph client key<span class="nt">&lt;/name&gt;</span>
<span class="nt">&lt;/usage&gt;</span>
<span class="nt">&lt;/secret&gt;</span>
</pre></div>
</div>
</li>
<li><p>Define a blank secret from this template.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo virsh secret-define --file ceph-secret.xml
</pre></div>
</div>
</li>
<li><p>Verify that the secret was created.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo virsh secret-list
</pre></div>
</div>
</li>
<li><p>Set the secret to the Ceph client’s key. [10]</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo virsh secret-set-value --secret &lt;GENERATED_UUID&gt; --base64 <span class="k">$(</span>ceph auth get-key client.&lt;USER&gt;<span class="k">)</span>
</pre></div>
</div>
</li>
<li><p>Finally, the secret needs to be referenced as type “ceph” with either
the “usage” (description) or “uuid” or the secret element that has
been created. [11]</p>
<div class="highlight-xml notranslate"><div class="highlight"><pre><span></span><span class="nt">&lt;devices&gt;</span>
<span class="nt">&lt;disk</span> <span class="na">type=</span><span class="s">&#39;network&#39;</span> <span class="na">device=</span><span class="s">&#39;disk&#39;</span><span class="nt">&gt;</span>
...
<span class="nt">&lt;auth</span> <span class="na">username=</span><span class="s">&#39;&lt;CLIENT&gt;&#39;</span><span class="nt">&gt;</span>
  <span class="nt">&lt;secret</span> <span class="na">type=</span><span class="s">&#39;ceph&#39;</span> <span class="na">usage=</span><span class="s">&#39;The Ceph client key&#39;</span><span class="nt">/&gt;</span>
<span class="nt">&lt;/auth&gt;</span>
...
<span class="nt">&lt;disk&gt;</span>
...
<span class="nt">&lt;/devices&gt;</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="cephfs">
<h2><a class="toc-backref" href="#id14">CephFS</a><a class="headerlink" href="#cephfs" title="Permalink to this headline">¶</a></h2>
<p>CephFS has been stable since the Ceph Jewel 10.2.0 release. This now
includes repair utilities, including fsck. For clients, it is
recommended to use a Linux kernel in the 4 series, or newer, to have the
latest features and bug fixes for the file system. [12]</p>
</div>
<div class="section" id="history">
<h2><a class="toc-backref" href="#id15">History</a><a class="headerlink" href="#history" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/ekultails/rootpages/commits/master/src/storage/ceph.rst">Latest</a></p></li>
<li><p><a class="reference external" href="https://github.com/ekultails/rootpages/commits/master/src/administration/file_systems.rst">&lt; 2019.07.01</a></p></li>
<li><p><a class="reference external" href="https://github.com/ekultails/rootpages/commits/master/src/file_systems.rst">&lt; 2019.01.01</a></p></li>
<li><p><a class="reference external" href="https://github.com/ekultails/rootpages/commits/master/markdown/file_systems.md">&lt; 2018.01.01</a></p></li>
</ul>
</div>
<div class="section" id="bibliography">
<h2><a class="toc-backref" href="#id16">Bibliography</a><a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Karan Singh <em>Learning Ceph</em> (Birmingham, UK: Packet Publishing, 2015)</p></li>
<li><p>“Ceph Jewel Preview: a new store is coming, BlueStore.” Sebastien Han. March 21, 2016. Accessed December 5, 2018. <a class="reference external" href="https://www.sebastien-han.fr/blog/2016/03/21/ceph-a-new-store-is-coming/">https://www.sebastien-han.fr/blog/2016/03/21/ceph-a-new-store-is-coming/</a></p></li>
<li><p>“CACHE POOL.” Ceph Documentation. Accessed January 19, 2017. <a class="reference external" href="http://docs.ceph.com/docs/jewel/dev/cache-pool/">http://docs.ceph.com/docs/jewel/dev/cache-pool/</a></p></li>
<li><p>“CEPHX CONFIG REFERENCE.” Ceph Documentation. Accessed January 28, 2017. <a class="reference external" href="http://docs.ceph.com/docs/master/rados/configuration/auth-config-ref/">http://docs.ceph.com/docs/master/rados/configuration/auth-config-ref/</a></p></li>
<li><p>“INTRO TO CEPH.” Ceph Documentation. Accessed January 15, 2017. <a class="reference external" href="http://docs.ceph.com/docs/jewel/start/intro/">http://docs.ceph.com/docs/jewel/start/intro/</a></p></li>
<li><p>“Ceph Deployment.” Ceph Jewel Documentation. Accessed January 14, 2017. <a class="reference external" href="http://docs.ceph.com/docs/jewel/rados/deployment/">http://docs.ceph.com/docs/jewel/rados/deployment/</a></p></li>
<li><p>“ceph-ansible Wiki.” ceph-ansible GitHub. February 29, 2016. Accessed January 15, 2017. <a class="reference external" href="https://github.com/ceph/ceph-ansible/wiki">https://github.com/ceph/ceph-ansible/wiki</a></p></li>
<li><p>“CRUSH MAPS.” Ceph Documentation. Accessed January 29, 2017. <a class="reference external" href="http://docs.ceph.com/docs/master/rados/operations/crush-map/">http://docs.ceph.com/docs/master/rados/operations/crush-map/</a></p></li>
<li><p>“Ceph: manually repair object.” April 27, 2015. Accessed January 15, 2017. <a class="reference external" href="http://ceph.com/planet/ceph-manually-repair-object/">http://ceph.com/planet/ceph-manually-repair-object/</a></p></li>
<li><p>“USING LIBVIRT WITH CEPH RBD.” Ceph Documentation. Accessed January 27, 2017. <a class="reference external" href="http://docs.ceph.com/docs/master/rbd/libvirt/">http://docs.ceph.com/docs/master/rbd/libvirt/</a></p></li>
<li><p>“Secret XML.” libvirt. Accessed January 27, 2017. <a class="reference external" href="https://libvirt.org/formatsecret.html">https://libvirt.org/formatsecret.html</a></p></li>
<li><p>“USING CEPHFS.” Ceph Documentation. Accessed January 15, 2017. <a class="reference external" href="http://docs.ceph.com/docs/master/cephfs/">http://docs.ceph.com/docs/master/cephfs/</a></p></li>
<li><p>“Ceph Releases (general)”. Ceph Documentation. July 27, 2020. Accessed August 13, 2020. <a class="reference external" href="https://docs.ceph.com/docs/master/releases/general/">https://docs.ceph.com/docs/master/releases/general/</a></p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="file_systems.html" class="btn btn-neutral float-right" title="File Systems" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="bootloaders.html" class="btn btn-neutral float-left" title="Bootloaders" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright None, Copyleft 2021, Luke Short. Documents licensed under GFDLv1.3.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>